{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbGzmlaYdaHF+jYkRCx0F5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saoudyahya/BioDataExtract/blob/main/BioDataExtract.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nUhA90ZY4B1",
        "outputId": "bf1e11e4-2793-419c-a33d-dff43be56f23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.23.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.6-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.2.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.29.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Collecting pdfminer.six==20250327 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250327-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.23.1-py3-none-any.whl (51.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfplumber-0.11.6-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250327-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading XlsxWriter-3.2.2-py3-none-any.whl (165 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, xlsxwriter, uvicorn, tomlkit, semantic-version, ruff, python-multipart, pypdfium2, groovy, ffmpy, aiofiles, starlette, safehttpx, pdfminer.six, gradio-client, fastapi, pdfplumber, gradio\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.23.1 gradio-client-1.8.0 groovy-0.1.2 pdfminer.six-20250327 pdfplumber-0.11.6 pydub-0.25.1 pypdfium2-4.30.1 python-multipart-0.0.20 ruff-0.11.2 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 uvicorn-0.34.0 xlsxwriter-3.2.2\n"
          ]
        }
      ],
      "source": [
        "pip install gradio pdfplumber spacy pandas xlsxwriter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import pdfplumber\n",
        "import spacy\n",
        "import re\n",
        "import pandas as pd\n",
        "import tempfile\n",
        "import os\n",
        "from io import BytesIO\n",
        "\n",
        "# Load the SpaCy model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"SpaCy model not found. Please download it using: python -m spacy download en_core_web_sm\")\n",
        "    nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Add scientific entity patterns\n",
        "ruler = nlp.add_pipe(\"entity_ruler\", name=\"entity_ruler\")\n",
        "patterns = [\n",
        "    {\"label\": \"SPECIES\", \"pattern\": [{\"LOWER\": {\"REGEX\": \"[a-z]+\"}}], \"id\": \"scientific_species\"},\n",
        "    {\"label\": \"MEASUREMENT\", \"pattern\": [{\"SHAPE\": \"d+.d+\"}, {\"LOWER\": {\"IN\": [\"mm\", \"cm\", \"m\", \"kg\", \"g\"]}}]},\n",
        "    {\"label\": \"LENGTH\", \"pattern\": [{\"LOWER\": \"fork\"}, {\"LOWER\": \"length\"}]},\n",
        "]\n",
        "ruler.add_patterns(patterns)\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(pdf_bytes):\n",
        "    try:\n",
        "        # Create a BytesIO object from the bytes\n",
        "        pdf_io = BytesIO(pdf_bytes)\n",
        "\n",
        "        text = \"\"\n",
        "        with pdfplumber.open(pdf_io) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                extracted_text = page.extract_text()\n",
        "                if extracted_text:\n",
        "                    text += extracted_text + \"\\n\"\n",
        "\n",
        "        if not text.strip():\n",
        "            return \"No text could be extracted from the PDF.\"\n",
        "\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        return f\"Error processing PDF: {str(e)}\"\n",
        "\n",
        "# Function to extract entities\n",
        "def extract_entities(text, entity_types):\n",
        "    if isinstance(text, str) and not text.startswith(\"Error\"):\n",
        "        try:\n",
        "            # Limit text size to avoid memory issues\n",
        "            text = text[:100000]  # Limit to first 100k characters\n",
        "            doc = nlp(text)\n",
        "\n",
        "            # Create a dictionary to store extracted entities\n",
        "            extracted = {entity_type: [] for entity_type in entity_types}\n",
        "\n",
        "            # Extract required entity types\n",
        "            for ent in doc.ents:\n",
        "                if ent.label_ in entity_types:\n",
        "                    extracted[ent.label_].append(ent.text)\n",
        "\n",
        "            # Special case for scientific species names\n",
        "            if \"SPECIES\" in entity_types:\n",
        "                scientific_names = re.findall(r'[A-Z][a-z]+ [a-z]+', text)\n",
        "                extracted[\"SPECIES\"].extend(scientific_names)\n",
        "\n",
        "            # Special case for measurements\n",
        "            if \"MEASUREMENT\" in entity_types:\n",
        "                measurements = re.findall(r'\\d+\\.?\\d*\\s*(?:mm|cm|m|kg|g)', text)\n",
        "                extracted[\"MEASUREMENT\"].extend(measurements)\n",
        "\n",
        "            # Special case for fork length\n",
        "            if \"LENGTH\" in entity_types:\n",
        "                fork_lengths = re.findall(r'(?:fork|total)?\\s*length\\s*(?:of)?\\s*\\d+\\.?\\d*\\s*(?:mm|cm|m)', text, re.IGNORECASE)\n",
        "                extracted[\"LENGTH\"].extend(fork_lengths)\n",
        "\n",
        "            # Remove duplicates\n",
        "            for entity_type in entity_types:\n",
        "                extracted[entity_type] = list(set(extracted[entity_type]))\n",
        "\n",
        "            return extracted\n",
        "        except Exception as e:\n",
        "            return f\"Error extracting entities: {str(e)}\"\n",
        "    else:\n",
        "        return f\"Cannot extract entities: {text}\"\n",
        "\n",
        "# Function for the Gradio interface\n",
        "def process_pdf(pdf_file, species, measurements, length, person, org, date, gpe, loc):\n",
        "    if pdf_file is None:\n",
        "        return \"Please upload a PDF file.\", None\n",
        "\n",
        "    # Get selected entity types\n",
        "    entity_types = []\n",
        "    if species: entity_types.append(\"SPECIES\")\n",
        "    if measurements: entity_types.append(\"MEASUREMENT\")\n",
        "    if length: entity_types.append(\"LENGTH\")\n",
        "    if person: entity_types.append(\"PERSON\")\n",
        "    if org: entity_types.append(\"ORG\")\n",
        "    if date: entity_types.append(\"DATE\")\n",
        "    if gpe: entity_types.append(\"GPE\")\n",
        "    if loc: entity_types.append(\"LOC\")\n",
        "\n",
        "    if not entity_types:\n",
        "        return \"Please select at least one entity type to extract.\", None\n",
        "\n",
        "    try:\n",
        "        # Process the PDF file (which is now in bytes format)\n",
        "        text = extract_text_from_pdf(pdf_file)\n",
        "\n",
        "        if isinstance(text, str) and text.startswith(\"Error\"):\n",
        "            return text, None\n",
        "\n",
        "        # Extract entities\n",
        "        extracted = extract_entities(text, entity_types)\n",
        "\n",
        "        if isinstance(extracted, str) and extracted.startswith(\"Error\"):\n",
        "            return extracted, None\n",
        "\n",
        "        # Prepare results\n",
        "        result_text = \"## Extracted Entities\\n\\n\"\n",
        "\n",
        "        try:\n",
        "            # Create Excel file with multiple sheets\n",
        "            output = BytesIO()\n",
        "            with pd.ExcelWriter(output, engine='xlsxwriter') as writer:\n",
        "                for entity_type in entity_types:\n",
        "                    if extracted[entity_type]:\n",
        "                        result_text += f\"### {entity_type}\\n\"\n",
        "                        for item in extracted[entity_type]:\n",
        "                            result_text += f\"- {item}\\n\"\n",
        "                        result_text += \"\\n\"\n",
        "\n",
        "                        # Add sheet to Excel file\n",
        "                        df = pd.DataFrame({entity_type: extracted[entity_type]})\n",
        "                        df.to_excel(writer, sheet_name=entity_type[:31], index=False)\n",
        "                    else:\n",
        "                        result_text += f\"### {entity_type}\\n\"\n",
        "                        result_text += \"No entities of this type found.\\n\\n\"\n",
        "\n",
        "            output.seek(0)\n",
        "            return result_text, output.getvalue()\n",
        "        except Exception as e:\n",
        "            return f\"Error creating Excel file: {str(e)}\", None\n",
        "    except Exception as e:\n",
        "        return f\"Unexpected error: {str(e)}\", None\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks(title=\"Scientific Entity Extraction Tool\") as demo:\n",
        "    gr.Markdown(\"# Scientific Entity Extraction Tool\")\n",
        "    gr.Markdown(\"Upload a scientific PDF and select entities to extract\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            # Input components - explicitly use binary format\n",
        "            pdf_input = gr.File(label=\"Upload PDF Document\", type=\"binary\")\n",
        "\n",
        "            gr.Markdown(\"### Select Entities to Extract\")\n",
        "            species_cb = gr.Checkbox(label=\"Species Names\", value=True)\n",
        "            measurements_cb = gr.Checkbox(label=\"Measurements\", value=True)\n",
        "            length_cb = gr.Checkbox(label=\"Length Measurements\", value=True)\n",
        "            person_cb = gr.Checkbox(label=\"Person Names\", value=False)\n",
        "            org_cb = gr.Checkbox(label=\"Organizations\", value=False)\n",
        "            date_cb = gr.Checkbox(label=\"Dates\", value=False)\n",
        "            gpe_cb = gr.Checkbox(label=\"Geopolitical Entities\", value=False)\n",
        "            loc_cb = gr.Checkbox(label=\"Locations\", value=False)\n",
        "\n",
        "            extract_button = gr.Button(\"Extract Entities\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            # Output components\n",
        "            output_text = gr.Markdown()\n",
        "            output_file = gr.File(label=\"Download Results\")\n",
        "\n",
        "    # Set up event handler\n",
        "    extract_button.click(\n",
        "        process_pdf,\n",
        "        inputs=[\n",
        "            pdf_input,\n",
        "            species_cb, measurements_cb, length_cb,\n",
        "            person_cb, org_cb, date_cb, gpe_cb, loc_cb\n",
        "        ],\n",
        "        outputs=[output_text, output_file]\n",
        "    )\n",
        "\n",
        "# Launch the app\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "vxtOYTZuY7Cq",
        "outputId": "e004b3df-6c5d-423b-dcde-d5db096be9d3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://2834885ef0729a827a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2834885ef0729a827a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import spacy\n",
        "import re\n",
        "import pandas as pd\n",
        "import tempfile\n",
        "import os\n",
        "from io import BytesIO\n",
        "\n",
        "# Load the SpaCy model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"SpaCy model not found. Please download it using: python -m spacy download en_core_web_sm\")\n",
        "    nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Add scientific entity patterns\n",
        "try:\n",
        "    ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "    patterns = [\n",
        "        {\"label\": \"SPECIES\", \"pattern\": [{\"LOWER\": {\"REGEX\": \"[a-z]+\"}}], \"id\": \"scientific_species\"},\n",
        "        {\"label\": \"MEASUREMENT\", \"pattern\": [{\"SHAPE\": \"d+.d+\"}, {\"LOWER\": {\"IN\": [\"mm\", \"cm\", \"m\", \"kg\", \"g\"]}}]},\n",
        "        {\"label\": \"LENGTH\", \"pattern\": [{\"LOWER\": \"fork\"}, {\"LOWER\": \"length\"}]},\n",
        "    ]\n",
        "    ruler.add_patterns(patterns)\n",
        "except Exception as e:\n",
        "    print(f\"Error setting up entity ruler: {e}\")\n",
        "    if \"entity_ruler\" not in nlp.pipe_names:\n",
        "        ruler = nlp.add_pipe(\"entity_ruler\", name=\"entity_ruler\")\n",
        "\n",
        "# Function to extract text from PDF with better error handling\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    if not pdf_path:\n",
        "        return \"No file provided.\"\n",
        "\n",
        "    try:\n",
        "        text = \"\"\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                extracted_text = page.extract_text()\n",
        "                if extracted_text:\n",
        "                    text += extracted_text + \"\\n\"\n",
        "\n",
        "        if not text.strip():\n",
        "            return \"No text could be extracted from the PDF.\"\n",
        "\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        return f\"Error processing PDF: {str(e)}\"\n",
        "\n",
        "# Function to extract entities with better error handling\n",
        "def extract_entities(text, entity_types):\n",
        "    if isinstance(text, str) and not text.startswith(\"Error\"):\n",
        "        try:\n",
        "            # Limit text size to avoid memory issues\n",
        "            text = text[:100000]  # Limit to first 100k characters\n",
        "            doc = nlp(text)\n",
        "\n",
        "            # Create a dictionary to store extracted entities\n",
        "            extracted = {entity_type: [] for entity_type in entity_types}\n",
        "\n",
        "            # Extract required entity types\n",
        "            for ent in doc.ents:\n",
        "                if ent.label_ in entity_types:\n",
        "                    extracted[ent.label_].append(ent.text)\n",
        "\n",
        "            # Special case for scientific species names\n",
        "            if \"SPECIES\" in entity_types:\n",
        "                scientific_names = re.findall(r'[A-Z][a-z]+ [a-z]+', text)\n",
        "                extracted[\"SPECIES\"].extend(scientific_names)\n",
        "\n",
        "            # Special case for measurements\n",
        "            if \"MEASUREMENT\" in entity_types:\n",
        "                measurements = re.findall(r'\\d+\\.?\\d*\\s*(?:mm|cm|m|kg|g)', text)\n",
        "                extracted[\"MEASUREMENT\"].extend(measurements)\n",
        "\n",
        "            # Special case for fork length\n",
        "            if \"LENGTH\" in entity_types:\n",
        "                fork_lengths = re.findall(r'(?:fork|total)?\\s*length\\s*(?:of)?\\s*\\d+\\.?\\d*\\s*(?:mm|cm|m)', text, re.IGNORECASE)\n",
        "                extracted[\"LENGTH\"].extend(fork_lengths)\n",
        "\n",
        "            # Remove duplicates\n",
        "            for entity_type in entity_types:\n",
        "                extracted[entity_type] = list(set(extracted[entity_type]))\n",
        "\n",
        "            return extracted\n",
        "        except Exception as e:\n",
        "            return f\"Error extracting entities: {str(e)}\"\n",
        "    else:\n",
        "        return f\"Cannot extract entities: {text}\"\n",
        "\n",
        "# Main function for processing PDFs\n",
        "def process_pdf(pdf_path, entity_types):\n",
        "    if not pdf_path:\n",
        "        return \"Please provide a PDF file path.\"\n",
        "\n",
        "    if not entity_types:\n",
        "        return \"Please specify at least one entity type to extract.\"\n",
        "\n",
        "    try:\n",
        "        # Extract text from PDF\n",
        "        text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "        print(f\"Extracted text length: {len(text) if isinstance(text, str) else 'Not a string'}\")\n",
        "\n",
        "        if isinstance(text, str) and text.startswith(\"Error\"):\n",
        "            return text, None\n",
        "\n",
        "        # Extract entities\n",
        "        extracted = extract_entities(text, entity_types)\n",
        "\n",
        "        if isinstance(extracted, str) and extracted.startswith(\"Error\"):\n",
        "            return extracted, None\n",
        "\n",
        "        # Prepare results\n",
        "        result_text = \"## Extracted Entities\\n\\n\"\n",
        "\n",
        "        try:\n",
        "            # Create Excel file with multiple sheets\n",
        "            output_file = \"extracted_entities.xlsx\"\n",
        "            with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:\n",
        "                for entity_type in entity_types:\n",
        "                    if extracted[entity_type]:\n",
        "                        result_text += f\"### {entity_type}\\n\"\n",
        "                        for item in extracted[entity_type]:\n",
        "                            result_text += f\"- {item}\\n\"\n",
        "                        result_text += \"\\n\"\n",
        "\n",
        "                        # Add sheet to Excel file\n",
        "                        df = pd.DataFrame({entity_type: extracted[entity_type]})\n",
        "                        df.to_excel(writer, sheet_name=entity_type[:31], index=False)\n",
        "                    else:\n",
        "                        result_text += f\"### {entity_type}\\n\"\n",
        "                        result_text += \"No entities of this type found.\\n\\n\"\n",
        "\n",
        "            print(result_text)\n",
        "            print(f\"Results saved to {output_file}\")\n",
        "            return result_text, output_file\n",
        "        except Exception as e:\n",
        "            return f\"Error creating Excel file: {str(e)}\", None\n",
        "    except Exception as e:\n",
        "        return f\"Unexpected error: {str(e)}\", None\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the path to your PDF file\n",
        "    pdf_path = input(\"Enter the path to your PDF file: \")\n",
        "\n",
        "    # Define entity types to extract\n",
        "    print(\"\\nSelect entity types to extract (enter y/n):\")\n",
        "    entity_types = []\n",
        "\n",
        "    if input(\"Species Names (y/n): \").lower() == 'y':\n",
        "        entity_types.append(\"SPECIES\")\n",
        "    if input(\"Measurements (y/n): \").lower() == 'y':\n",
        "        entity_types.append(\"MEASUREMENT\")\n",
        "    if input(\"Length Measurements (y/n): \").lower() == 'y':\n",
        "        entity_types.append(\"LENGTH\")\n",
        "    if input(\"Person Names (y/n): \").lower() == 'y':\n",
        "        entity_types.append(\"PERSON\")\n",
        "    if input(\"Organizations (y/n): \").lower() == 'y':\n",
        "        entity_types.append(\"ORG\")\n",
        "    if input(\"Dates (y/n): \").lower() == 'y':\n",
        "        entity_types.append(\"DATE\")\n",
        "    if input(\"Geopolitical Entities (y/n): \").lower() == 'y':\n",
        "        entity_types.append(\"GPE\")\n",
        "    if input(\"Locations (y/n): \").lower() == 'y':\n",
        "        entity_types.append(\"LOC\")\n",
        "\n",
        "    # Process the PDF\n",
        "    result, output_file = process_pdf(pdf_path, entity_types)\n",
        "\n",
        "    # Display the result\n",
        "    print(\"\\nResult:\")\n",
        "    print(result)\n",
        "    if output_file:\n",
        "        print(f\"Results saved to: {output_file}\")YYYYY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YB8pUUXzGE_n",
        "outputId": "ac0ec96c-40fd-4373-f221-f91e61d49205"
      },
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the path to your PDF file: /content/1706.03762v7 (1).pdf\n",
            "\n",
            "Select entity types to extract (enter y/n):\n",
            "Species Names (y/n): Y\n",
            "Measurements (y/n): Y\n",
            "Length Measurements (y/n): Y\n",
            "Person Names (y/n): \n",
            "Organizations (y/n): Y\n",
            "Dates (y/n): \n",
            "Geopolitical Entities (y/n): \n",
            "Locations (y/n): \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted text length: 35526\n",
            "## Extracted Entities\n",
            "\n",
            "### SPECIES\n",
            "- For translation\n",
            "- amount\n",
            "- scholarlyworks\n",
            "- Forthebigmodels\n",
            "- Head\n",
            "- performing\n",
            "- ls\n",
            "- si\n",
            "- attentionmechanism\n",
            "- normalization\n",
            "- theoutputofthepreviouslayerinthe\n",
            "- range\n",
            "- Differentcolorsrepresentdifferentheads\n",
            "- Wesetthemaximumoutputlengthduring\n",
            "- evaluate\n",
            "- andWO\n",
            "- separable\n",
            "- accurate\n",
            "- a√ndvaluesofdimensiond\n",
            "- guA\n",
            "- WSJonly\n",
            "- inner\n",
            "- attentionovertheoutputoftheencoderstack\n",
            "- mationandsoftmaxfunctiontoconvertthedecoderoutputtopredictednext\n",
            "- transduction\n",
            "- Proceedings\n",
            "- between\n",
            "- dispensingwithrecurrenceandconvolutions\n",
            "- right\n",
            "- ofthesoftmaxwhichcorrespondtoillegalconnections\n",
            "- arXivpreprintarXiv:1511.06114,2015\n",
            "- implemented\n",
            "- new\n",
            "- aswellastheembedding\n",
            "- Adam\n",
            "- languagemodelingtasks[34\n",
            "- end\n",
            "- params\n",
            "- replacingtherecurrentlayersmostcommonlyusedinencoder\n",
            "- JamieRyanKiros\n",
            "- QKT\n",
            "- WMT\n",
            "- MultiHead(Q\n",
            "- fordifferentlayertypes\n",
            "- continuous\n",
            "- Whilethetwoaresimilarintheoreticalcomplexity\n",
            "- asmallfractionofthetrainingcostsofthe\n",
            "- reducedtoaconstantnumberofoperations\n",
            "- Need\n",
            "- Thefirstisamulti\n",
            "- Empiricalevaluation\n",
            "- TheTransformerfollowsthisoverallarchitectureusingstackedself\n",
            "- Ateachstepthemodelisauto\n",
            "- pages152–159.ACL\n",
            "- ProductAttention\n",
            "- Recurrent\n",
            "- tahw\n",
            "- ThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n",
            "- alsoinlayer5of6,apparentlyinvolvedinanaphoraresolution\n",
            "- propose\n",
            "- Additiveattentioncomputesthecompatibilityfunctionusingafeed\n",
            "- German dataset\n",
            "- andZbigniewWojna\n",
            "- head\n",
            "- outperformingallofthepreviouslypublishedsinglemodels\n",
            "- besuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\n",
            "- artBLEUscoreof28.4\n",
            "- constraints\n",
            "- beforeitisaddedtothe\n",
            "- These are\n",
            "- mechanisms\n",
            "- Inaself\n",
            "- sometimescalledintra\n",
            "- ourmodel\n",
            "- largeandlimitedtrainingdata\n",
            "- Sinceourmodelcontainsnorecurrenceandnoconvolution\n",
            "- duringtraining\n",
            "- Adecomposableattention\n",
            "- InAdvancesinNeuralInformationProcessingSystems\n",
            "- sentencepairs\n",
            "- termmemory[13]andgatedrecurrent[7]neuralnetworks\n",
            "- depictedinFigure2\n",
            "- forthesemi\n",
            "- connectedlayersforboththeencoderanddecoder\n",
            "- Each\n",
            "- O(log\n",
            "- Asingleconvolutionallayerwithkernelwidthk\n",
            "- Table2\n",
            "- trainingforparsing\n",
            "- complexity\n",
            "- Weinspectattentiondistributions\n",
            "- Exploring\n",
            "- again\n",
            "- dap\n",
            "- pos+k\n",
            "- attentioncouldberestrictedtoconsideringonlyaneighborhoodofsizerin\n",
            "- weusearateof\n",
            "- projected\n",
            "- QuocLe\n",
            "- inferencetoinputlength+50,butterminateearlywhenpossible[38\n",
            "- inthiscase\n",
            "- best\n",
            "- learn\n",
            "- queriesandkeysofdimensiond\n",
            "- several\n",
            "- Optimizer\n",
            "- ensembles\n",
            "- Multi\n",
            "- Attentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\n",
            "- this\n",
            "- AsnotedinTable1,aself\n",
            "- mechanism\n",
            "- atlessthan1/4thetrainingcostofthe\n",
            "- JianpengCheng\n",
            "- tasks\n",
            "- Experiments on\n",
            "- Transformer can\n",
            "- thedotproductsgrowlargeinmagnitude\n",
            "- While single\n",
            "- Thepenntreebank\n",
            "- more\n",
            "- MaximumPathLength\n",
            "- wordperplexities\n",
            "- AdvancesinNeuralInformationProcessingSystems28,pages2440–2448.CurranAssociates\n",
            "- tionmodelsinvarioustasks\n",
            "- example\n",
            "- During\n",
            "- i=1\n",
            "- effect\n",
            "- werechosenafterexperimentationonthedevelopmentset\n",
            "- QuocV.Le\n",
            "- tionallayerscommonlyusedformappingonevariable\n",
            "- Forourbigmodels,(describedonthe\n",
            "- sequencemodels[37],theTransformeroutperformstheBerkeley-\n",
            "- i\n",
            "- wise\n",
            "- used\n",
            "- thedifficultyoflearninglong\n",
            "- ŁukaszKaiser∗\n",
            "- German\n",
            "- -dimensional\n",
            "- The dimensionality\n",
            "- Petrovetal\n",
            "- resulting\n",
            "- self\n",
            "- yieldingbetterresultsthanallpreviouslyreportedmodelswiththeexceptionofthe\n",
            "- theyusedifferentparameters\n",
            "- learnedandfixed[9\n",
            "- weaveragedthelast20checkpoints\n",
            "- Thesparsely\n",
            "- Thiswouldincreasethemaximum\n",
            "- dropoutrateP\n",
            "- Listingorderisrandom\n",
            "- Vision and\n",
            "- Theoutputiscomputedasaweightedsum\n",
            "- sinusoidalpositionalencodingwithlearnedpositionalembeddings[9],andobservenearlyidentical\n",
            "- WeusedtheAdamoptimizer[20]withβ\n",
            "- weusedthesignificantlylargerWMT\n",
            "- lrate\n",
            "- n))inthecaseofdilatedconvolutions[18\n",
            "- Similartotheencoder\n",
            "- O(n\n",
            "- over\n",
            "- HeadAttention\n",
            "- Is\n",
            "- Transformer this\n",
            "- illia.polosukhin@gmail.com\n",
            "- AshishVaswani∗\n",
            "- stack\n",
            "- Assidebenefit\n",
            "- t−1\n",
            "- wordpiece\n",
            "- endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\n",
            "- min(step_num−0.5,step_num·warmup_steps−1.5\n",
            "- establishinganewstate\n",
            "- weapplydropouttothesumsoftheembeddingsandthe\n",
            "- othertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\n",
            "- tuning\n",
            "- Table1\n",
            "- sentencerepresentationsusedbystate\n",
            "- basedneuralmachinetranslation\n",
            "- Bestviewedincolor\n",
            "- with\n",
            "- Attention(Q\n",
            "- we\n",
            "- described\n",
            "- Similarly\n",
            "- representation\n",
            "- asexpected\n",
            "- TheTransformergeneralizeswelltoEnglishconstituencyparsing(ResultsareonSection23\n",
            "- andBeatriceSantorini\n",
            "- artmodelsinmachinetranslations\n",
            "- k.\n",
            "- tub\n",
            "- detail\n",
            "- averaginginhibitsthis\n",
            "- andQuocVVLe\n",
            "- andQuocV.Le\n",
            "- GeoffreyEHinton\n",
            "- Wefurtherobserveinrows(C)and(D)that\n",
            "- andV.\n",
            "- PE\n",
            "- theinputsequencecenteredaroundtherespectiveoutputposition\n",
            "- often\n",
            "- tensorflow\n",
            "- End\n",
            "- additiveattentionoutperforms\n",
            "- Motivatingouruseofself\n",
            "- Frenchdatasetconsistingof36Msentencesandsplittokensintoa32000word\n",
            "- Wevariedthelearning\n",
            "- qualityalsodropsoffwithtoomanyheads\n",
            "- Neural\n",
            "- preprintarXiv:1610.02357,2016\n",
            "- Canactivememoryreplaceattention\n",
            "- whichbecomescriticalatlonger\n",
            "- ofasinglesequenceinordertocomputearepresentationofthesequence\n",
            "- LayerType\n",
            "- pos\n",
            "- MoE[32\n",
            "- resultstothebasemodel\n",
            "- Acknowledgements\n",
            "- attentionwe\n",
            "- hidden\n",
            "- LayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub\n",
            "- parallel\n",
            "- decrease\n",
            "- Vinyals&Kaiserelal\n",
            "- Usingtheoutputembeddingtoimprovelanguagemodels\n",
            "- distherepresentationdimension\n",
            "- Frenchtranslationtask\n",
            "- comments\n",
            "- subsequent\n",
            "- ourmodelestablishesanewsingle\n",
            "- The code\n",
            "- That\n",
            "- orderofthesequence\n",
            "- pathlengthtoO(n\n",
            "- lack\n",
            "- Figure\n",
            "- architecturesfromtheliterature\n",
            "- attentionisanattentionmechanismrelatingdifferentpositions\n",
            "- term\n",
            "- naciremA\n",
            "- discriminative\n",
            "- layersineachencoderlayer\n",
            "- structured\n",
            "- weighted\n",
            "- model\n",
            "- Attention(restricted\n",
            "- Unlistedvaluesareidenticaltothoseofthebase\n",
            "- tirips\n",
            "- Transformer(big\n",
            "- Adeepreinforcedmodelforabstractive\n",
            "- areusedinconjunctionwitharecurrentnetwork\n",
            "- verylongsequences\n",
            "- semi\n",
            "- InAdvancesinNeural\n",
            "- regressive\n",
            "- attentionhasbeen\n",
            "- modelarchitecture\n",
            "- suchasword\n",
            "- thequeriescomefromthepreviousdecoderlayer\n",
            "- despite\n",
            "- versionsproducednearlyidenticalresults(seeTable3row(E)).Wechosethesinusoidalversion\n",
            "- rangedependenciesinthenetwork\n",
            "- modeloutperformsevenallpreviouslyreportedensembles\n",
            "- InC.Cortes\n",
            "- artresultsinsmall\n",
            "- Massiveexplorationofneural\n",
            "- sentence\n",
            "- ToevaluateiftheTransformercangeneralizetoothertasksweperformedexperimentsonEnglish\n",
            "- has\n",
            "- listedinthebottomlineofTable3\n",
            "- keeping\n",
            "- way\n",
            "- b\n",
            "- thenumberofGPUsused\n",
            "- forwardnetworkwith\n",
            "- by\n",
            "- Intheembeddinglayers\n",
            "- These\n",
            "- supervised\n",
            "- tokensandoutputtokenstovectorsofdimensiond\n",
            "- Layernormalization\n",
            "- taht\n",
            "- etal\n",
            "- andGeoffreyEHinton\n",
            "- decoderforstatistical\n",
            "- andRichardSocher\n",
            "- WheretheprojectionsareparametermatricesWQ\n",
            "- weadd\"positionalencodings\"totheinputembeddingsatthe\n",
            "- multi\n",
            "- symbol\n",
            "- network\n",
            "- thecompetitivemodels\n",
            "- Hencewealsocompare\n",
            "- llion@google.com\n",
            "- Nogueira dos\n",
            "- alignedRNNsorconvolution\n",
            "- asafunctionoftheprevioushiddenstateh\n",
            "- bottomsoftheencoderanddecoderstacks\n",
            "- He\n",
            "- ‡WorkperformedwhileatGoogleResearch\n",
            "- reduceconstituentparsing\n",
            "- andanestimateofthesustained\n",
            "- layersinthemodel\n",
            "- targettokens\n",
            "- B\n",
            "- implementingtensor2tensor\n",
            "- tokensinthesequence\n",
            "- Similarlytoothersequencetransductionmodels\n",
            "- our\n",
            "- Whileforsmallvaluesofd\n",
            "- Theinputconsistsof\n",
            "- insideofscaleddot\n",
            "- ScaledDot\n",
            "- maps\n",
            "- Wecomputethedotproductsofthe\n",
            "- translation\n",
            "- show\n",
            "- Attention\n",
            "- aroundeachofthesub\n",
            "- faster\n",
            "- thelimitsoflanguagemodeling\n",
            "- remains\n",
            "- which\n",
            "- andAlexandraBirch\n",
            "- Theheadsclearlylearnedtoperformdifferenttasks\n",
            "- Inthefollowingsections\n",
            "- headself\n",
            "- tluciffid\n",
            "- DennyBritz\n",
            "- forbothWSJonlyandthesemi\n",
            "- The encoder\n",
            "- thehyperparametersdescribedthroughoutthepaper\n",
            "- Wetraineda4\n",
            "- positionalembeddinginsteadofsinusoids\n",
            "- nov\n",
            "- Attention(QWQ\n",
            "- heads\n",
            "- Thatis\n",
            "- noitacilppa\n",
            "- GermanandEnglish\n",
            "- modelstate\n",
            "- Transformer is\n",
            "- Frenchused\n",
            "- Fullattentionsforhead5\n",
            "- unchanged\n",
            "- networks\n",
            "- inTable2)outperformsthebestpreviouslyreportedmodels(includingensembles)bymorethan2.0\n",
            "- interpretable\n",
            "- ni\n",
            "- ff\n",
            "- gnissim\n",
            "- BLEU\n",
            "- executed\n",
            "- attentive\n",
            "- 1024ontheWallStreetJournal(WSJ)portionofthe\n",
            "- arXivpreprintarXiv:1602.02410,2016\n",
            "- aspects\n",
            "- Long short\n",
            "- asmeasuredbytheminimumnumberofsequentialoperationsrequired\n",
            "- abilitytolearnsuchdependenciesisthelengthofthepathsforwardandbackwardsignalshaveto\n",
            "- consistsoftwolineartransformationswithaReLUactivationinbetween\n",
            "- trainingfor3.5daysoneightGPUs\n",
            "- differentlayertypes\n",
            "- to\n",
            "- stnemnrevog\n",
            "- Thebest\n",
            "- andshouldnotbecomparedto\n",
            "- Given z\n",
            "- existing\n",
            "- minuteintervals\n",
            "- Figure1\n",
            "- input\n",
            "- each\n",
            "- attentioninlayer5of6\n",
            "- convolutionisequaltothecombinationofaself\n",
            "- output\n",
            "- nikip@google.com\n",
            "- respectively\n",
            "- andd\n",
            "- exceptforthescalingfactor\n",
            "- smaller\n",
            "- allsub\n",
            "- attentionlayerandapoint\n",
            "- Thesehyperparameters\n",
            "- computation\n",
            "- Another\n",
            "- Deep\n",
            "- NikiParmar∗\n",
            "- wherehead\n",
            "- left\n",
            "- MaryAnnMarcinkiewicz\n",
            "- D\n",
            "- yieldingd\n",
            "- can\n",
            "- significantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\n",
            "- artmodel\n",
            "- checkpointaveraging\n",
            "- time\n",
            "- than\n",
            "- sti\n",
            "- modelshavenotbeenabletoattainstate\n",
            "- learningtoalignandtranslate\n",
            "- connectedfeed\n",
            "- For\n",
            "- ŁukaszKaiserandSamyBengio\n",
            "- dluohs\n",
            "- Separable convolutions\n",
            "- suchattentionmechanisms\n",
            "- pairencoding\n",
            "- machinetranslationarchitectures\n",
            "- predictionsforpositionicandependonlyontheknownoutputsatpositionslessthani\n",
            "- JakobUszkoreit∗\n",
            "- arXivpreprintarXiv:1705.03122v2,2017\n",
            "- constant\n",
            "- Therearemanychoicesofpositionalencodings\n",
            "- relyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput\n",
            "- Thebigmodelsweretrainedfor300,000steps\n",
            "- ∈Rhdv×dmodel\n",
            "- weachieveanewstateoftheart\n",
            "- extremelysmallgradients4\n",
            "- NoamShazeer∗\n",
            "- pages832–841.ACL\n",
            "- an\n",
            "- Inadditiontoattentionsub\n",
            "- produceoutputsofdimensiond\n",
            "- sinceforanyfixedoffsetk\n",
            "- whereas\n",
            "- To\n",
            "- Thethirdisthepathlengthbetweenlong\n",
            "- not\n",
            "- Weusedavocabularyof16KtokensfortheWSJonlysettingandavocabularyof32Ktokens\n",
            "- tree\n",
            "- gnitov\n",
            "- havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n",
            "- Petrov\n",
            "- toO(k·n·d+n·d2\n",
            "- long\n",
            "- RafalJozefowicz\n",
            "- WearegratefultoNalKalchbrennerandStephanGouwsfortheirfruitful\n",
            "- block\n",
            "- InICLR,2015\n",
            "- ym\n",
            "- base\n",
            "- values\n",
            "- at\n",
            "- InProc.ofNAACL,2016\n",
            "- Inaddition\n",
            "- August2009\n",
            "- suchasimages\n",
            "- Thewavelengthsformageometricprogressionfrom2πto10000·2π\n",
            "- supervisedsetting\n",
            "- regressiveproperty\n",
            "- InTable3rows(A),wevarythenumberofattentionheadsandtheattentionkeyandvaluedimensions\n",
            "- Parser[29]evenwhentrainingonlyontheWSJtrainingsetof40Ksentences\n",
            "- theverb‘making’,completingthephrase‘making\n",
            "- whereposisthepositionandiisthedimension\n",
            "- Duetothereduceddimensionofeachhead\n",
            "- increasingthelengthofthelongestpaths\n",
            "- Fast\n",
            "- WO\n",
            "- forwardnetwork\n",
            "- M.Sugiyama\n",
            "- cid:80)dk\n",
            "- Bottom\n",
            "- ZhongqiangHuangandMaryHarper\n",
            "- acrosslanguages\n",
            "- bestmodelsfromtheliterature\n",
            "- corpusofenglish\n",
            "- werewrittenat10\n",
            "- allotherparameters\n",
            "- recognition\n",
            "- Operations\n",
            "- Table4\n",
            "- dataset\n",
            "- Francois\n",
            "- attentionandstarted\n",
            "- Transformer(basemodel\n",
            "- the\n",
            "- once\n",
            "- arXivpreprintarXiv:1705.04304,2017\n",
            "- trainingPCFGgrammarswithlatentannotations\n",
            "- plantoextendtheTransformertoproblemsinvolvinginputandoutputmodalitiesotherthantextand\n",
            "- representations\n",
            "- consists\n",
            "- Vinyals&Kaiser\n",
            "- andR.Garnett\n",
            "- indifferentways\n",
            "- Frenchtranslationtasks\n",
            "- ofgatedrecurrentneuralnetworksonsequencemodeling\n",
            "- tI\n",
            "- consisting\n",
            "- Proceedings of\n",
            "- simple\n",
            "- linearlyforConvS2SandlogarithmicallyforByteNet\n",
            "- equal\n",
            "- headsclearlylearntoperformdifferenttasks\n",
            "- statesh\n",
            "- textualentailmentandlearningtask\n",
            "- LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\n",
            "- sin(pos/100002i\n",
            "- eachdimensionofthepositionalencoding\n",
            "- andsemanticstructureofthesentences\n",
            "- Training\n",
            "- andLukaszKaiser\n",
            "- big\n",
            "- attentionlayerallofthekeys\n",
            "- layertransformerwithd\n",
            "- dessap\n",
            "- n\n",
            "- and\n",
            "- Weimplementthis\n",
            "- rateoverthecourseoftraining\n",
            "- sequential\n",
            "- Generating\n",
            "- Weusedbeamsearchasdescribedintheprevioussection\n",
            "- be\n",
            "- TrainingCost(FLOPs\n",
            "- Table2summarizesourresultsandcomparesourtranslationqualityandtrainingcoststoothermodel\n",
            "- computinghiddenrepresentationsinparallelforallinputandoutputpositions\n",
            "- nisthesequencelength\n",
            "- sequence(y\n",
            "- arXiv\n",
            "- fromtwodifferentheadsfromtheencoderself\n",
            "- audioandvideo\n",
            "- Deep residual\n",
            "- d\n",
            "- themaximumpathlengthbetweenanytwoinputandoutputpositionsinnetworkscomposedofthe\n",
            "- attentionlayers\n",
            "- dimensions\n",
            "- P\n",
            "- JournalofMachine\n",
            "- valuesandqueries\n",
            "- encoder\n",
            "- requires\n",
            "- moredifficult\n",
            "- bottomlineoftable3),steptimewas1.0seconds\n",
            "- layerinputandnormalized\n",
            "- Each layer\n",
            "- theoutputissubjecttostrongstructural\n",
            "- MachineTranslation\n",
            "- positioninthedecodertoattendoverallpositionsintheinputsequence\n",
            "- Learning\n",
            "- counteract\n",
            "- sequence\n",
            "- memory\n",
            "- whichisappliedtoeachpositionseparatelyandidentically\n",
            "- trained\n",
            "- Weneedtopreventleftward\n",
            "- sothatthetwocanbesummed\n",
            "- lukaszkaiser@google.com\n",
            "- productattentionbymaskingout(settingto−∞)allvaluesintheinput\n",
            "- describedinsection3.2\n",
            "- pointcapacityofeachGPU5\n",
            "- Separable\n",
            "- effortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder\n",
            "- determining\n",
            "- sequencelengths\n",
            "- usedsuccessfullyinavarietyoftasksincludingreadingcomprehension\n",
            "- InInternationalConference\n",
            "- eht\n",
            "- DoingsorequiresastackofO(n\n",
            "- fully\n",
            "- Dot\n",
            "- architectures\n",
            "- Weareexcitedaboutthefutureofattention\n",
            "- on\n",
            "- K\n",
            "- transductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5\n",
            "- it\n",
            "- per\n",
            "- MitchellPMarcus\n",
            "- editors\n",
            "- SergeyIoffe\n",
            "- longer\n",
            "- Our model\n",
            "- efficientinferenceandvisualizations\n",
            "- another\n",
            "- andtheinputforpositiont\n",
            "- Here\n",
            "- precisionfloating\n",
            "- In the\n",
            "- a\n",
            "- andJakobUszkoreit\n",
            "- siht\n",
            "- itself\n",
            "- References\n",
            "- attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9\n",
            "- EnglishConstituencyParsing\n",
            "- Neuralmachinetranslationofrarewords\n",
            "- learning\n",
            "- task\n",
            "- albeitatthecostofreducedeffectiveresolutiondue\n",
            "- ofthevalues\n",
            "- knowledge\n",
            "- size\n",
            "- results\n",
            "- VariationsontheTransformerarchitecture\n",
            "- entirelyonself\n",
            "- usingthelargerhigh\n",
            "- Huang&Harper(2009)[14\n",
            "- https://github.com/\n",
            "- Forourbasemodelsusing\n",
            "- recurrentnets\n",
            "- eachtrainingsteptookabout0.4seconds\n",
            "- sequences\n",
            "- increasedthemaximumoutputlengthtoinputlength+300\n",
            "- basedsolelyonattentionmechanisms\n",
            "- weuselearnedembeddingstoconverttheinput\n",
            "- traverseinthenetwork\n",
            "- computationalcomplexity\n",
            "- weusedasinglemodelobtainedbyaveragingthelast5checkpoints\n",
            "- final\n",
            "- Weemployaresidualconnection[11]aroundeachof\n",
            "- thetwomechanismsperformsimilarly\n",
            "- V)=Concat(head\n",
            "- sur-\n",
            "- machinetranslation\n",
            "- sizeofconvolutionsandrthesizeoftheneighborhoodinrestrictedself\n",
            "- abs/1412.3555,2014\n",
            "- InformationProcessingSystems,(NIPS),2016\n",
            "- pages3104–3112,2014\n",
            "- summarization\n",
            "- ssecorp\n",
            "- h\n",
            "- divideeachby\n",
            "- networkgrammars\n",
            "- Deep recurrent\n",
            "- Maximumpathlengths\n",
            "- basedmodelsandplantoapplythemtoothertasks\n",
            "- Evenourbasemodel\n",
            "- ndoesnotconnectallpairsofinputandoutput\n",
            "- InEmpiricalMethodsinNaturalLanguageProcessing,2016\n",
            "- andqueriescomefromthesameplace\n",
            "- tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\n",
            "- Attention as\n",
            "- DavidMcClosky\n",
            "- generates\n",
            "- short\n",
            "- machine\n",
            "- v\n",
            "- andAlexanderM.Rush\n",
            "- We trained\n",
            "- JakobproposedreplacingRNNswithself\n",
            "- positionalencodingsinboththeencoderanddecoderstacks\n",
            "- Xiv preprint\n",
            "- layer\n",
            "- layercomplexityandminimumnumberofsequentialoperations\n",
            "- dataregimes[37\n",
            "- ro\n",
            "- †WorkperformedwhileatGoogleBrain\n",
            "- constraintofsequentialcomputation\n",
            "- Inthiswork\n",
            "- andChristopherDManning\n",
            "- Recognition\n",
            "- PennTreebank[25],about40Ktrainingsentences\n",
            "- train\n",
            "- aidan@cs.toronto.edu\n",
            "- Numerous\n",
            "- dotproductattentionwithoutscalingforlargervaluesofd\n",
            "- relativepositions\n",
            "- single\n",
            "- E\n",
            "- including\n",
            "- remained\n",
            "- AttentionVisualizations\n",
            "- or\n",
            "- erom\n",
            "- amodelarchitectureeschewingrecurrenceandinstead\n",
            "- learned\n",
            "- case\n",
            "- convolutional\n",
            "- keys\n",
            "- inparticular\n",
            "- q·k=\n",
            "- Pattern\n",
            "- Thedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\n",
            "- allpositionsinthedecoderuptoandincludingthatposition\n",
            "- through\n",
            "- theinputoroutputsequences[2,19\n",
            "- InProceedingsofthe2009ConferenceonEmpiricalMethodsinNatural\n",
            "- are\n",
            "- wesharethesameweightmatrixbetweenthetwoembeddinglayersandthepre-√softmax\n",
            "- andthememorykeysandvaluescomefromtheoutputoftheencoder\n",
            "- LearningResearch,15(1):1929–1958,2014\n",
            "- TheTransformerachievesbetterBLEUscoresthanpreviousstate\n",
            "- hurtsperplexity\n",
            "- waL\n",
            "- Recentworkhasachieved\n",
            "- recurrent\n",
            "- IncontrasttoRNNsequence\n",
            "- forwardlayer\n",
            "- sequentialnatureprecludesparallelizationwithintrainingexamples\n",
            "- independentsentencerepresentations[4,27,28,22\n",
            "- Wealsousetheusuallearnedlineartransfor-\n",
            "- butno\n",
            "- Kaiming\n",
            "- Computationallinguistics,19(2):313–330,1993\n",
            "- attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\n",
            "- drop\n",
            "- Learningphraserepresentationsusingrnnencoder\n",
            "- ensuresthatthe\n",
            "- productattentionis\n",
            "- 4Toillustratewhythedotproductsgetlarge\n",
            "- wewilldescribetheTransformer\n",
            "- wherethequery\n",
            "- astheembeddings\n",
            "- N\n",
            "- rangedependencies[12\n",
            "- Convolutionallayersaregenerallymoreexpensivethan\n",
            "- Abstract\n",
            "- The\n",
            "- Onekeyfactoraffectingthe\n",
            "- Manyoftheattentionheadsexhibitbehaviourthatseemsrelatedtothestructureofthe\n",
            "- kernel\n",
            "- tionalsequencetosequencelearning\n",
            "- ourresearch\n",
            "- fromourmodelsandpresentanddiscussexamplesintheappendix\n",
            "- LC.sc\n",
            "- ToevaluatetheimportanceofdifferentcomponentsoftheTransformer\n",
            "- While\n",
            "- confidenceandBerkleyParsercorporafromwithapproximately17Msentences\n",
            "- Anattentionfunctioncanbedescribedasmappingaqueryandasetofkey\n",
            "- A\n",
            "- Inthesemodels\n",
            "- pairencoding[3],whichhasasharedsource-\n",
            "- embedding\n",
            "- artmodelsonthe\n",
            "- eachofthelayersinourencoderanddecodercontainsafully\n",
            "- operations\n",
            "- RecurrentNeuralNetworkGrammar[8\n",
            "- attentionlayersarefasterthanrecurrentlayerswhenthesequence\n",
            "- Eachpositionintheencodercanattendtoallpositionsinthepreviouslayerofthe\n",
            "- /h=64\n",
            "- describing\n",
            "- Encoder\n",
            "- biggermodelsarebetter\n",
            "- We\n",
            "- thematrixofoutputsas\n",
            "- motivate\n",
            "- networksformachine\n",
            "- noitartsiger\n",
            "- thecomplexityofaseparable\n",
            "- followed\n",
            "- Intheformertaskourbest\n",
            "- based\n",
            "- Thismakes\n",
            "- distance\n",
            "- Figure5\n",
            "- convolutions\n",
            "- models\n",
            "- wemustinjectsomeinformationabouttherelativeorabsolutepositionofthe\n",
            "- productattentionisidenticaltoouralgorithm\n",
            "- butimprovesaccuracyandBLEUscore\n",
            "- O(1\n",
            "- We propose\n",
            "- August2013\n",
            "- considerably\n",
            "- O(n2·d\n",
            "- Structuredattentionnetworks\n",
            "- In\"encoder\n",
            "- followedbylayernormalization\n",
            "- tsuj\n",
            "- shownintheleftandrighthalvesofFigure1\n",
            "- compact\n",
            "- orO(log\n",
            "- architecture\n",
            "- decoderarchitectureswith\n",
            "- In\n",
            "- Sequential\n",
            "- atafractionofthetrainingcostofanyof\n",
            "- q\n",
            "- and6\n",
            "- JonasGehring\n",
            "- wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\n",
            "- FFN(x)=max(0,xW\n",
            "- Effectiveself\n",
            "- Germantranslationonthe\n",
            "- To the\n",
            "- Our\n",
            "- O(r·n·d\n",
            "- PositionalEncoding\n",
            "- correspondstoasinusoid\n",
            "- Section\n",
            "- Furthermore\n",
            "- forwardconnectionsforneuralmachinetranslation\n",
            "- in\n",
            "- Wealsomodifytheself\n",
            "- attentionlayersrunninginparallel\n",
            "- beparallelized\n",
            "- Weusedabeamsizeof21andα=0.3\n",
            "- trainedthebasemodelsforatotalof100,000stepsor12hours\n",
            "- productattention\n",
            "- dependenciesisakeychallengeinmanysequencetransductiontasks\n",
            "- Listed\n",
            "- Frenchnewstest2014testsatafractionofthetrainingcost\n",
            "- RomainPaulus\n",
            "- asimplewaytopreventneuralnetworksfromoverfitting\n",
            "- InInternationalConferenceonLearningRepresentations,2017\n",
            "- FactorizationtricksforLSTMnetworks\n",
            "- During inference\n",
            "- That is\n",
            "- querywiththecorrespondingkey\n",
            "- abs/1512.00567,2015\n",
            "- andJeffDean\n",
            "- designedandimplementedthefirstTransformermodelsand\n",
            "- generative\n",
            "- NeuralGPUslearnalgorithms\n",
            "- connect\n",
            "- Inpractice\n",
            "- GNMT+RL[38\n",
            "- inference\n",
            "- also\n",
            "- Thiscorrespondstoincreasingthelearningratelinearlyforthefirstwarmup_stepstrainingsteps\n",
            "- allowingmodelingofdependencieswithoutregardtotheirdistancein\n",
            "- targetvocabularyofabout37000tokens\n",
            "- On both\n",
            "- Decoder\n",
            "- Attentionshereshownonlyfor\n",
            "- averaging\n",
            "- annotation\n",
            "- greatlyimprovingresultsandmassivelyaccelerating\n",
            "- difficult\n",
            "- dependencies\n",
            "- performs\n",
            "- InProceedingsofthe51stAnnualMeetingoftheACL(Volume\n",
            "- combinedwithfactthattheoutputembeddingsareoffsetbyoneposition\n",
            "- noam@google.com\n",
            "- andoutputsequences\n",
            "- Thetwomostcommonlyusedattentionfunctionsareadditiveattention[2],anddot\n",
            "- era\n",
            "- avaswani@google.com\n",
            "- encoderself\n",
            "- noinipo\n",
            "- bothattentionandresidual\n",
            "- relying\n",
            "- querywithallkeys\n",
            "- IlliaPolosukhin∗\n",
            "- V)=softmax\n",
            "- use\n",
            "- On\n",
            "- Isolatedattentionsfromjusttheword‘its’forattentionheads5\n",
            "- tokenprobabilities\n",
            "- freepositionrepresentationandbecametheotherpersoninvolvedinnearlyevery\n",
            "- DiederikKingmaandJimmyBa\n",
            "- 0.98andϵ=10−9\n",
            "- composed\n",
            "- Attention consists\n",
            "- wepresentedtheTransformer\n",
            "- attending\n",
            "- thetotalcomputationalcost\n",
            "- sequencetosequencelearning\n",
            "- compare\n",
            "- arXivpreprintarXiv:1701.06538,2017\n",
            "- andapplyasoftmaxfunctiontoobtaintheweightsonthe\n",
            "- 31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach\n",
            "- Amethodforstochasticoptimization\n",
            "- canberepresentedasalinearfunctionof\n",
            "- t\n",
            "- fitting\n",
            "- modelbymultiplyingthetrainingtime\n",
            "- Top\n",
            "- ProductAttention\"(Figure2\n",
            "- andoutputareallvectors\n",
            "- Nikidesigned\n",
            "- ∈Rdmodel×dv\n",
            "- product(multi-\n",
            "- Effectiveapproachestoattention-\n",
            "- You\n",
            "- pages433–440.ACL\n",
            "- tcefrep\n",
            "- weemployresidualconnections\n",
            "- Rethinkingtheinceptionarchitectureforcomputervision\n",
            "- Given\n",
            "- thenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\n",
            "- efficientinpractice\n",
            "- Wealsoexperimentedwithusinglearnedpositionalembeddings[9]instead\n",
            "- Whileforsmallvaluesofd thetwomechanismsperformsimilarly\n",
            "- of\n",
            "- kisthekernel\n",
            "- andthesecondisasimple\n",
            "- as\n",
            "- thedecoderinsertsathirdsub\n",
            "- pushingthesoftmaxfunctionintoregionswhereithas\n",
            "- Trainingtook3.5dayson8P100GPUs\n",
            "- however\n",
            "- wefounditbeneficialtolinearlyprojectthequeries\n",
            "- dev\n",
            "- arXiv:1703.10722,2017\n",
            "- Thisallowsevery\n",
            "- arXiv:1703.03130,2017\n",
            "- OfirPressandLiorWolf\n",
            "- translationsystem\n",
            "- Our results\n",
            "- Rd\n",
            "- for\n",
            "- thebigtransformermodel(Transformer(big\n",
            "- attentionis0.9BLEUworsethanthebestsetting\n",
            "- swal\n",
            "- tensor2tensor\n",
            "- Convolu-\n",
            "- ∗Equalcontribution\n",
            "- arXiv:1308.0850,2013\n",
            "- andYannN.Dauphin\n",
            "- Grammarasaforeignlanguage\n",
            "- AllmetricsareontheEnglish\n",
            "- Generating sequences\n",
            "- anddecreasingitthereafterproportionallytotheinversesquarerootofthestepnumber\n",
            "- attentionandpoint\n",
            "- arXivpreprintarXiv:1508.04025,2015\n",
            "- is\n",
            "- ModelArchitecture\n",
            "- sophisticated\n",
            "- subspacesatdifferentpositions\n",
            "- cos(pos/100002i\n",
            "- experts\n",
            "- Wecompute\n",
            "- these\n",
            "- attentionandtheparameter\n",
            "- assumethatthecomponentsofqandkareindependentrandom\n",
            "- headattentionallowsthemodeltojointlyattendtoinformationfromdifferentrepresentation\n",
            "- wescalethedotproductsby\n",
            "- various\n",
            "- standard\n",
            "- Thismimicsthe\n",
            "- Transformer(4layers\n",
            "- prevent\n",
            "- achieves\n",
            "- wecomputetheattentionfunctiononasetofqueriessimultaneously\n",
            "- matrixmultiplicationcode\n",
            "- asmemoryconstraintslimitbatchingacrossexamples\n",
            "- OleksiiKuchaievandBorisGinsburg\n",
            "- both\n",
            "- asthemodellearnstobemoreunsure\n",
            "- keysandvalueshtimeswithdifferent\n",
            "- Thistaskpresentsspecificchallenges\n",
            "- residual\n",
            "- computation[32],whilealsoimprovingmodelperformanceincaseofthelatter\n",
            "- dos\n",
            "- less\n",
            "- Experiments\n",
            "- fast\n",
            "- ehT\n",
            "- TrainingDataandBatching\n",
            "- hasbeencruciallyinvolvedineveryaspectofthiswork\n",
            "- specific\n",
            "- ConvS2S[9\n",
            "- theefforttoevaluatethisidea\n",
            "- headedself\n",
            "- ConvS2SEnsemble[9\n",
            "- EncoderandDecoderStacks\n",
            "- German translation\n",
            "- ofsymbolsoneelementatatime\n",
            "- This\n",
            "- positions\n",
            "- informationflowinthedecodertopreservetheauto\n",
            "- k)convolutionallayersinthecaseofcontiguouskernels\n",
            "- ŁukaszKaiserandIlyaSutskever\n",
            "- Fast and\n",
            "- Theshorterthesepathsbetweenanycombinationofpositionsintheinput\n",
            "- Noamproposedscaleddot\n",
            "- Inallbutafewcases[27],however\n",
            "- ThedecoderisalsocomposedofastackofN\n",
            "- Inadditiontothetwo\n",
            "- becauseitmayallowthemodeltoextrapolatetosequencelengthslongerthantheonesencountered\n",
            "- plicative)attention\n",
            "- Germantranslationdevelopmentset\n",
            "- hurtsmodelquality\n",
            "- work\n",
            "- compatibility\n",
            "- Weperformedonlyasmallnumberofexperimentstoselectthedropout\n",
            "- Aligningthepositionstostepsincomputationtime\n",
            "- Wesuspectthatforlargevaluesof\n",
            "- O(n·d2\n",
            "- decoderstructure[5,2,35\n",
            "- variableswithmean0andvariance1.Thentheirdotproduct\n",
            "- ApplicationsofAttentioninourModel\n",
            "- Table3\n",
            "- lineartransformation\n",
            "- age\n",
            "- ecnis\n",
            "- Another way\n",
            "- significantly\n",
            "- Thefundamental\n",
            "- position-\n",
            "- gatedmixture\n",
            "- attention\n",
            "- lengthsequenceofsymbolrepresentations\n",
            "- ytirojam\n",
            "- Providedproperattributionisprovided\n",
            "- Llionalsoexperimentedwithnovelmodelvariants\n",
            "- previousstate\n",
            "- wasresponsibleforourinitialcodebase\n",
            "- In this\n",
            "- section5.4),learningratesandbeamsizeontheSection22developmentset\n",
            "- restrictedattentionmechanismstoefficientlyhandlelargeinputsandoutputs\n",
            "- termdependencies,2001\n",
            "- pos,2i+1\n",
            "- thefirstsequencetransductionmodelbasedentirelyon\n",
            "- artBLEUscoreof41.8after\n",
            "- wisefeed\n",
            "- neural\n",
            "- wisefullyconnectedfeed\n",
            "- translationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs\n",
            "- VWV\n",
            "- betweenanytwopositionsinthenetwork\n",
            "- gnikam\n",
            "- theapproachwetakeinourmodel\n",
            "- code\n",
            "- steps\n",
            "- andRuslanSalakhutdi-\n",
            "- Att+PosUnk[39\n",
            "- functionthandotproductmaybebeneficial\n",
            "- AdvancesinNeuralInformationProcessingSystems,2015\n",
            "- abs/1406.1078,2014\n",
            "- Wecallourparticularattention\"ScaledDot\n",
            "- Notonlydoindividualattention\n",
            "- reading\n",
            "- attentioncouldyieldmoreinterpretablemodels\n",
            "- Withasingleattentionhead\n",
            "- fo\n",
            "- constituencyparsing\n",
            "- vocabulary[38].Sentencepairswerebatchedtogetherbyapproximatesequencelength\n",
            "- Results\n",
            "- decoder\n",
            "- manyappeartoexhibitbehaviorrelatedtothesyntactic\n",
            "- Weplantoinvestigatethisapproachfurtherinfuturework\n",
            "- Evenwithk\n",
            "- issimilartothatofsingle\n",
            "- usedbeamsearchwithabeamsizeof4andlengthpenaltyα\n",
            "- sinceitcanbeimplementedusinghighlyoptimized\n",
            "- Xception\n",
            "- Thisinherently\n",
            "- headattentioninthreedifferentways\n",
            "- weusesineandcosinefunctionsofdifferentfrequencies\n",
            "- accordingtoourbyte\n",
            "- preprint\n",
            "- OntheWMT2014English\n",
            "- All\n",
            "- alignedrecurrenceandhavebeenshowntoperformwellonsimple\n",
            "- then\n",
            "- Conference on\n",
            "- Self\n",
            "- Dropout\n",
            "- Neural computation\n",
            "- dot\n",
            "- weemployedlabelsmoothingofvalueϵ\n",
            "- WeshowthattheTransformergeneralizeswellto\n",
            "- dmodel\n",
            "- wemultiplythoseweightsby\n",
            "- ourbigmodelachievesaBLEUscoreof41.0\n",
            "- available\n",
            "- architectures[38,24,15\n",
            "- Layer5\n",
            "- Whilethelineartransformationsarethesameacrossdifferentpositions\n",
            "- Hochreiter and\n",
            "- TheTransformer(big)modeltrainedforEnglish\n",
            "- developmentset\n",
            "- masking\n",
            "- typical\n",
            "- Scaled\n",
            "- ThekeysandvaluesarealsopackedtogetherintomatricesK\n",
            "- replacingourearliercodebase\n",
            "- considerthreedesiderata\n",
            "- ModelVariations\n",
            "- Makinggenerationlesssequentialisanotherresearchgoalsofours\n",
            "- Bridgingthegapbetweenhumanandmachinetranslation\n",
            "- shift\n",
            "- pos,2i\n",
            "- ∈Rdmodel×dk\n",
            "- convolutionalneuralnetworksthatincludeanencoderandadecoder\n",
            "- C\n",
            "- Recurrent neural\n",
            "- 5Weusedvaluesof2.8,3.7,6.0and9.5TFLOPSforK80,K40,M40andP100,respectively\n",
            "- entirely\n",
            "- Background\n",
            "- Weemploythreetypesofregularizationduringtraining\n",
            "- surpassesallpreviouslypublishedmodelsandensembles\n",
            "- anddropoutisveryhelpfulinavoidingover\n",
            "- Chollet\n",
            "- Figure4\n",
            "- toinvestigatelocal\n",
            "- employ\n",
            "- German and\n",
            "- Weused\n",
            "- Long\n",
            "- identical\n",
            "- WhySelf\n",
            "- from\n",
            "- piece\n",
            "- ForwardNetworks\n",
            "- withsubwordunits\n",
            "- Outrageouslylargeneuralnetworks\n",
            "- An\n",
            "- andMarkJohnson\n",
            "- Forthebasemodel\n",
            "- wevariedourbasemodel\n",
            "- easy\n",
            "- layers\n",
            "- inthedistancebetweenpositions\n",
            "- m\n",
            "- correctionsandinspiration\n",
            "- depthwise\n",
            "- ProceedingsoftheHumanLanguageTechnologyConferenceoftheNAACL\n",
            "- An example\n",
            "- theygenerateasequenceofhidden\n",
            "- ew\n",
            "- warmup_steps=4000\n",
            "- German base\n",
            "- factor\n",
            "- byover2BLEU.OntheWMT2014English\n",
            "- Tothisend\n",
            "- SOE\n",
            "- following\n",
            "- Deep learning\n",
            "- reproducethetablesandfiguresinthispapersolelyforuseinjournalisticor\n",
            "- im-\n",
            "- Learning accurate\n",
            "- Wegivetwosuchexamplesabove\n",
            "- valuepairstoanoutput\n",
            "- suggests\n",
            "- andYoshuaBengio\n",
            "- whichperformsmulti\n",
            "- attentionlayersinthedecoderalloweachpositioninthedecodertoattendto\n",
            "- Inc.\n",
            "- decoderattention\"layers\n",
            "- hasmean0andvarianced\n",
            "- theeasieritistolearnlong\n",
            "- Toimprovecomputationalperformancefortasksinvolving\n",
            "- Sequencetosequencelearningwithneural\n",
            "- English-\n",
            "- distant\n",
            "- such\n",
            "- attentionlayerconnectsallpositionswithaconstantnumberofsequentially\n",
            "- improving\n",
            "- prisinglywell\n",
            "- Google’sneuralmachine\n",
            "- Theconfigurationofthismodelis\n",
            "- languagequestionansweringand\n",
            "- arXivpreprint\n",
            "- TheTransformer\n",
            "- Twoattentionheads\n",
            "- section\n",
            "- z\n",
            "- Buildingalargeannotated\n",
            "- dimensionality\n",
            "- andYonghuiWu\n",
            "- Tocounteractthiseffect\n",
            "- GNMT+RLEnsemble[38\n",
            "- r\n",
            "- most\n",
            "- LanguageProcessing\n",
            "- perplexitiesareper\n",
            "- concatenated\n",
            "- Neuralmachinetranslationbyjointly\n",
            "- theword‘making\n",
            "- In terms\n",
            "- chosethisfunctionbecausewehypothesizeditwouldallowthemodeltoeasilylearntoattendby\n",
            "- length\n",
            "- ofWSJ\n",
            "- Position\n",
            "- InTable3rows(B),weobservethatreducingtheattentionkeysized\n",
            "- WepresenttheseresultsinTable3\n",
            "- sub\n",
            "- x\n",
            "- abstractivesummarization\n",
            "- linearprojectionstod\n",
            "- V\n",
            "- k\n",
            "- packedtogether\n",
            "- layerinatypicalsequencetransductionencoderordecoder\n",
            "- Weestimatethenumberoffloatingpointoperationsusedtotraina\n",
            "- abs/1703.03906,2017\n",
            "- termmemory\n",
            "- terms\n",
            "- y\n",
            "- GoogleBrain\n",
            "- Mostcompetitiveneuralsequencetransductionmodelshaveanencoder\n",
            "- inorderforthemodeltomakeuseofthe\n",
            "- muchfasterandmorespace\n",
            "- Regularization\n",
            "- that\n",
            "- headattentionwithfulldimensionality\n",
            "- convolu-\n",
            "- wiseFeed\n",
            "- For each\n",
            "\n",
            "### MEASUREMENT\n",
            "- 1 m\n",
            "- 3202\n",
            "g\n",
            "- 4.5 m\n",
            "- 9002\n",
            "g\n",
            "- 512.\n",
            "m\n",
            "\n",
            "### LENGTH\n",
            "No entities of this type found.\n",
            "\n",
            "### ORG\n",
            "- atlayer5of6\n",
            "- Deep-Att+PosUnkEnsemble[39\n",
            "- PPL BLEU\n",
            "- DzmitryBahdanau\n",
            "- SeppHochreiter\n",
            "- Sepp Hochreiter\n",
            "- MikeSchuster\n",
            "- Notethattheattentionsareverysharpforthisword\n",
            "- AnnaGoldie\n",
            "- MaximKrikun\n",
            "- Convolutional O(k·n·d2\n",
            "- WMT\n",
            "- KyunghyunCho\n",
            "- Zhuetal\n",
            "- NoamShazeer\n",
            "- YuanCao\n",
            "- YoshuaBengio\n",
            "- AlexGraves\n",
            "- Multi-Head Attention\n",
            "- DipanjanDas\n",
            "- Dyeretal\n",
            "- YoonKim\n",
            "- Input-Input\n",
            "- JunyoungChung\n",
            "- GoogleBrain GoogleBrain GoogleResearch GoogleResearch\n",
            "- CaglarGulcehre\n",
            "- BLEU\n",
            "- Minh-ThangLuong\n",
            "- OriolVinyals\n",
            "- HolgerSchwenk\n",
            "- McCloskyetal\n",
            "- EmbeddingsandSoftmax\n",
            "- Ashish,withIllia\n",
            "- Forthebasemodels\n",
            "- GoogleResearch UniversityofToronto\n",
            "- CoRR\n",
            "- TheTransformerallowsforsignificantlymoreparallelizationandcanreachanewstateoftheartin\n",
            "- KarenSimonyan\n",
            "- the IEEE Conference on Computer Vision\n",
            "- CaimingXiong\n",
            "- HieuPham\n",
            "- AnkurParikh\n",
            "- andMirellaLapata\n",
            "- CarlDenton\n",
            "- AlexKrizhevsky\n",
            "- RicoSennrich\n",
            "- NalKalchbrenner\n",
            "- CA\n",
            "- LabelSmoothing Duringtraining\n",
            "- AaronvandenOord\n",
            "- AzaliaMirhoseini\n",
            "- Dot-Product Attention\n",
            "- BarryHaddow\n",
            "- KrzysztofMaziarz\n",
            "- LasseEspeholt\n",
            "- the 21st International Conference on\n",
            "ComputationalLinguisticsand44thAnnualMeetingoftheACL\n",
            "- ChristianSzegedy\n",
            "- Manyoftheattentionheadsattendtoadistantdependencyof\n",
            "- TheTransformerusesmulti\n",
            "- Luongetal\n",
            "- OscarTäckström\n",
            "- N d\n",
            "- AndyDavis\n",
            "- D.D.Lee\n",
            "- QinGao\n",
            "- BartvanMerrienboer\n",
            "- Thissectiondescribesthetrainingregimeforourmodels\n",
            "- KlausMacherey\n",
            "- PE\n",
            "- RNN\n",
            "- ComplexityperLayer\n",
            "- NitishSrivastava\n",
            "- InthisworkweproposetheTransformer\n",
            "- ÇaglarGülçehre\n",
            "- LuongHoang\n",
            "- 7 Conclusion\n",
            "Inthiswork\n",
            "- FethiBougares\n",
            "- DavidGrangier\n",
            "- MainConference\n",
            "- Neuralmachinetranslationinlineartime.arXivpreprintarXiv:1610.10099v2\n",
            "- Transformer\n",
            "- VincentVanhoucke\n",
            "- DenisYarats\n",
            "- MichaelAuli\n",
            "- KWK\n",
            "- IlyaSutskever\n",
            "- PaoloFrasconi\n",
            "\n",
            "\n",
            "Results saved to extracted_entities.xlsx\n",
            "\n",
            "Result:\n",
            "## Extracted Entities\n",
            "\n",
            "### SPECIES\n",
            "- For translation\n",
            "- amount\n",
            "- scholarlyworks\n",
            "- Forthebigmodels\n",
            "- Head\n",
            "- performing\n",
            "- ls\n",
            "- si\n",
            "- attentionmechanism\n",
            "- normalization\n",
            "- theoutputofthepreviouslayerinthe\n",
            "- range\n",
            "- Differentcolorsrepresentdifferentheads\n",
            "- Wesetthemaximumoutputlengthduring\n",
            "- evaluate\n",
            "- andWO\n",
            "- separable\n",
            "- accurate\n",
            "- a√ndvaluesofdimensiond\n",
            "- guA\n",
            "- WSJonly\n",
            "- inner\n",
            "- attentionovertheoutputoftheencoderstack\n",
            "- mationandsoftmaxfunctiontoconvertthedecoderoutputtopredictednext\n",
            "- transduction\n",
            "- Proceedings\n",
            "- between\n",
            "- dispensingwithrecurrenceandconvolutions\n",
            "- right\n",
            "- ofthesoftmaxwhichcorrespondtoillegalconnections\n",
            "- arXivpreprintarXiv:1511.06114,2015\n",
            "- implemented\n",
            "- new\n",
            "- aswellastheembedding\n",
            "- Adam\n",
            "- languagemodelingtasks[34\n",
            "- end\n",
            "- params\n",
            "- replacingtherecurrentlayersmostcommonlyusedinencoder\n",
            "- JamieRyanKiros\n",
            "- QKT\n",
            "- WMT\n",
            "- MultiHead(Q\n",
            "- fordifferentlayertypes\n",
            "- continuous\n",
            "- Whilethetwoaresimilarintheoreticalcomplexity\n",
            "- asmallfractionofthetrainingcostsofthe\n",
            "- reducedtoaconstantnumberofoperations\n",
            "- Need\n",
            "- Thefirstisamulti\n",
            "- Empiricalevaluation\n",
            "- TheTransformerfollowsthisoverallarchitectureusingstackedself\n",
            "- Ateachstepthemodelisauto\n",
            "- pages152–159.ACL\n",
            "- ProductAttention\n",
            "- Recurrent\n",
            "- tahw\n",
            "- ThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n",
            "- alsoinlayer5of6,apparentlyinvolvedinanaphoraresolution\n",
            "- propose\n",
            "- Additiveattentioncomputesthecompatibilityfunctionusingafeed\n",
            "- German dataset\n",
            "- andZbigniewWojna\n",
            "- head\n",
            "- outperformingallofthepreviouslypublishedsinglemodels\n",
            "- besuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\n",
            "- artBLEUscoreof28.4\n",
            "- constraints\n",
            "- beforeitisaddedtothe\n",
            "- These are\n",
            "- mechanisms\n",
            "- Inaself\n",
            "- sometimescalledintra\n",
            "- ourmodel\n",
            "- largeandlimitedtrainingdata\n",
            "- Sinceourmodelcontainsnorecurrenceandnoconvolution\n",
            "- duringtraining\n",
            "- Adecomposableattention\n",
            "- InAdvancesinNeuralInformationProcessingSystems\n",
            "- sentencepairs\n",
            "- termmemory[13]andgatedrecurrent[7]neuralnetworks\n",
            "- depictedinFigure2\n",
            "- forthesemi\n",
            "- connectedlayersforboththeencoderanddecoder\n",
            "- Each\n",
            "- O(log\n",
            "- Asingleconvolutionallayerwithkernelwidthk\n",
            "- Table2\n",
            "- trainingforparsing\n",
            "- complexity\n",
            "- Weinspectattentiondistributions\n",
            "- Exploring\n",
            "- again\n",
            "- dap\n",
            "- pos+k\n",
            "- attentioncouldberestrictedtoconsideringonlyaneighborhoodofsizerin\n",
            "- weusearateof\n",
            "- projected\n",
            "- QuocLe\n",
            "- inferencetoinputlength+50,butterminateearlywhenpossible[38\n",
            "- inthiscase\n",
            "- best\n",
            "- learn\n",
            "- queriesandkeysofdimensiond\n",
            "- several\n",
            "- Optimizer\n",
            "- ensembles\n",
            "- Multi\n",
            "- Attentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\n",
            "- this\n",
            "- AsnotedinTable1,aself\n",
            "- mechanism\n",
            "- atlessthan1/4thetrainingcostofthe\n",
            "- JianpengCheng\n",
            "- tasks\n",
            "- Experiments on\n",
            "- Transformer can\n",
            "- thedotproductsgrowlargeinmagnitude\n",
            "- While single\n",
            "- Thepenntreebank\n",
            "- more\n",
            "- MaximumPathLength\n",
            "- wordperplexities\n",
            "- AdvancesinNeuralInformationProcessingSystems28,pages2440–2448.CurranAssociates\n",
            "- tionmodelsinvarioustasks\n",
            "- example\n",
            "- During\n",
            "- i=1\n",
            "- effect\n",
            "- werechosenafterexperimentationonthedevelopmentset\n",
            "- QuocV.Le\n",
            "- tionallayerscommonlyusedformappingonevariable\n",
            "- Forourbigmodels,(describedonthe\n",
            "- sequencemodels[37],theTransformeroutperformstheBerkeley-\n",
            "- i\n",
            "- wise\n",
            "- used\n",
            "- thedifficultyoflearninglong\n",
            "- ŁukaszKaiser∗\n",
            "- German\n",
            "- -dimensional\n",
            "- The dimensionality\n",
            "- Petrovetal\n",
            "- resulting\n",
            "- self\n",
            "- yieldingbetterresultsthanallpreviouslyreportedmodelswiththeexceptionofthe\n",
            "- theyusedifferentparameters\n",
            "- learnedandfixed[9\n",
            "- weaveragedthelast20checkpoints\n",
            "- Thesparsely\n",
            "- Thiswouldincreasethemaximum\n",
            "- dropoutrateP\n",
            "- Listingorderisrandom\n",
            "- Vision and\n",
            "- Theoutputiscomputedasaweightedsum\n",
            "- sinusoidalpositionalencodingwithlearnedpositionalembeddings[9],andobservenearlyidentical\n",
            "- WeusedtheAdamoptimizer[20]withβ\n",
            "- weusedthesignificantlylargerWMT\n",
            "- lrate\n",
            "- n))inthecaseofdilatedconvolutions[18\n",
            "- Similartotheencoder\n",
            "- O(n\n",
            "- over\n",
            "- HeadAttention\n",
            "- Is\n",
            "- Transformer this\n",
            "- illia.polosukhin@gmail.com\n",
            "- AshishVaswani∗\n",
            "- stack\n",
            "- Assidebenefit\n",
            "- t−1\n",
            "- wordpiece\n",
            "- endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\n",
            "- min(step_num−0.5,step_num·warmup_steps−1.5\n",
            "- establishinganewstate\n",
            "- weapplydropouttothesumsoftheembeddingsandthe\n",
            "- othertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\n",
            "- tuning\n",
            "- Table1\n",
            "- sentencerepresentationsusedbystate\n",
            "- basedneuralmachinetranslation\n",
            "- Bestviewedincolor\n",
            "- with\n",
            "- Attention(Q\n",
            "- we\n",
            "- described\n",
            "- Similarly\n",
            "- representation\n",
            "- asexpected\n",
            "- TheTransformergeneralizeswelltoEnglishconstituencyparsing(ResultsareonSection23\n",
            "- andBeatriceSantorini\n",
            "- artmodelsinmachinetranslations\n",
            "- k.\n",
            "- tub\n",
            "- detail\n",
            "- averaginginhibitsthis\n",
            "- andQuocVVLe\n",
            "- andQuocV.Le\n",
            "- GeoffreyEHinton\n",
            "- Wefurtherobserveinrows(C)and(D)that\n",
            "- andV.\n",
            "- PE\n",
            "- theinputsequencecenteredaroundtherespectiveoutputposition\n",
            "- often\n",
            "- tensorflow\n",
            "- End\n",
            "- additiveattentionoutperforms\n",
            "- Motivatingouruseofself\n",
            "- Frenchdatasetconsistingof36Msentencesandsplittokensintoa32000word\n",
            "- Wevariedthelearning\n",
            "- qualityalsodropsoffwithtoomanyheads\n",
            "- Neural\n",
            "- preprintarXiv:1610.02357,2016\n",
            "- Canactivememoryreplaceattention\n",
            "- whichbecomescriticalatlonger\n",
            "- ofasinglesequenceinordertocomputearepresentationofthesequence\n",
            "- LayerType\n",
            "- pos\n",
            "- MoE[32\n",
            "- resultstothebasemodel\n",
            "- Acknowledgements\n",
            "- attentionwe\n",
            "- hidden\n",
            "- LayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub\n",
            "- parallel\n",
            "- decrease\n",
            "- Vinyals&Kaiserelal\n",
            "- Usingtheoutputembeddingtoimprovelanguagemodels\n",
            "- distherepresentationdimension\n",
            "- Frenchtranslationtask\n",
            "- comments\n",
            "- subsequent\n",
            "- ourmodelestablishesanewsingle\n",
            "- The code\n",
            "- That\n",
            "- orderofthesequence\n",
            "- pathlengthtoO(n\n",
            "- lack\n",
            "- Figure\n",
            "- architecturesfromtheliterature\n",
            "- attentionisanattentionmechanismrelatingdifferentpositions\n",
            "- term\n",
            "- naciremA\n",
            "- discriminative\n",
            "- layersineachencoderlayer\n",
            "- structured\n",
            "- weighted\n",
            "- model\n",
            "- Attention(restricted\n",
            "- Unlistedvaluesareidenticaltothoseofthebase\n",
            "- tirips\n",
            "- Transformer(big\n",
            "- Adeepreinforcedmodelforabstractive\n",
            "- areusedinconjunctionwitharecurrentnetwork\n",
            "- verylongsequences\n",
            "- semi\n",
            "- InAdvancesinNeural\n",
            "- regressive\n",
            "- attentionhasbeen\n",
            "- modelarchitecture\n",
            "- suchasword\n",
            "- thequeriescomefromthepreviousdecoderlayer\n",
            "- despite\n",
            "- versionsproducednearlyidenticalresults(seeTable3row(E)).Wechosethesinusoidalversion\n",
            "- rangedependenciesinthenetwork\n",
            "- modeloutperformsevenallpreviouslyreportedensembles\n",
            "- InC.Cortes\n",
            "- artresultsinsmall\n",
            "- Massiveexplorationofneural\n",
            "- sentence\n",
            "- ToevaluateiftheTransformercangeneralizetoothertasksweperformedexperimentsonEnglish\n",
            "- has\n",
            "- listedinthebottomlineofTable3\n",
            "- keeping\n",
            "- way\n",
            "- b\n",
            "- thenumberofGPUsused\n",
            "- forwardnetworkwith\n",
            "- by\n",
            "- Intheembeddinglayers\n",
            "- These\n",
            "- supervised\n",
            "- tokensandoutputtokenstovectorsofdimensiond\n",
            "- Layernormalization\n",
            "- taht\n",
            "- etal\n",
            "- andGeoffreyEHinton\n",
            "- decoderforstatistical\n",
            "- andRichardSocher\n",
            "- WheretheprojectionsareparametermatricesWQ\n",
            "- weadd\"positionalencodings\"totheinputembeddingsatthe\n",
            "- multi\n",
            "- symbol\n",
            "- network\n",
            "- thecompetitivemodels\n",
            "- Hencewealsocompare\n",
            "- llion@google.com\n",
            "- Nogueira dos\n",
            "- alignedRNNsorconvolution\n",
            "- asafunctionoftheprevioushiddenstateh\n",
            "- bottomsoftheencoderanddecoderstacks\n",
            "- He\n",
            "- ‡WorkperformedwhileatGoogleResearch\n",
            "- reduceconstituentparsing\n",
            "- andanestimateofthesustained\n",
            "- layersinthemodel\n",
            "- targettokens\n",
            "- B\n",
            "- implementingtensor2tensor\n",
            "- tokensinthesequence\n",
            "- Similarlytoothersequencetransductionmodels\n",
            "- our\n",
            "- Whileforsmallvaluesofd\n",
            "- Theinputconsistsof\n",
            "- insideofscaleddot\n",
            "- ScaledDot\n",
            "- maps\n",
            "- Wecomputethedotproductsofthe\n",
            "- translation\n",
            "- show\n",
            "- Attention\n",
            "- aroundeachofthesub\n",
            "- faster\n",
            "- thelimitsoflanguagemodeling\n",
            "- remains\n",
            "- which\n",
            "- andAlexandraBirch\n",
            "- Theheadsclearlylearnedtoperformdifferenttasks\n",
            "- Inthefollowingsections\n",
            "- headself\n",
            "- tluciffid\n",
            "- DennyBritz\n",
            "- forbothWSJonlyandthesemi\n",
            "- The encoder\n",
            "- thehyperparametersdescribedthroughoutthepaper\n",
            "- Wetraineda4\n",
            "- positionalembeddinginsteadofsinusoids\n",
            "- nov\n",
            "- Attention(QWQ\n",
            "- heads\n",
            "- Thatis\n",
            "- noitacilppa\n",
            "- GermanandEnglish\n",
            "- modelstate\n",
            "- Transformer is\n",
            "- Frenchused\n",
            "- Fullattentionsforhead5\n",
            "- unchanged\n",
            "- networks\n",
            "- inTable2)outperformsthebestpreviouslyreportedmodels(includingensembles)bymorethan2.0\n",
            "- interpretable\n",
            "- ni\n",
            "- ff\n",
            "- gnissim\n",
            "- BLEU\n",
            "- executed\n",
            "- attentive\n",
            "- 1024ontheWallStreetJournal(WSJ)portionofthe\n",
            "- arXivpreprintarXiv:1602.02410,2016\n",
            "- aspects\n",
            "- Long short\n",
            "- asmeasuredbytheminimumnumberofsequentialoperationsrequired\n",
            "- abilitytolearnsuchdependenciesisthelengthofthepathsforwardandbackwardsignalshaveto\n",
            "- consistsoftwolineartransformationswithaReLUactivationinbetween\n",
            "- trainingfor3.5daysoneightGPUs\n",
            "- differentlayertypes\n",
            "- to\n",
            "- stnemnrevog\n",
            "- Thebest\n",
            "- andshouldnotbecomparedto\n",
            "- Given z\n",
            "- existing\n",
            "- minuteintervals\n",
            "- Figure1\n",
            "- input\n",
            "- each\n",
            "- attentioninlayer5of6\n",
            "- convolutionisequaltothecombinationofaself\n",
            "- output\n",
            "- nikip@google.com\n",
            "- respectively\n",
            "- andd\n",
            "- exceptforthescalingfactor\n",
            "- smaller\n",
            "- allsub\n",
            "- attentionlayerandapoint\n",
            "- Thesehyperparameters\n",
            "- computation\n",
            "- Another\n",
            "- Deep\n",
            "- NikiParmar∗\n",
            "- wherehead\n",
            "- left\n",
            "- MaryAnnMarcinkiewicz\n",
            "- D\n",
            "- yieldingd\n",
            "- can\n",
            "- significantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\n",
            "- artmodel\n",
            "- checkpointaveraging\n",
            "- time\n",
            "- than\n",
            "- sti\n",
            "- modelshavenotbeenabletoattainstate\n",
            "- learningtoalignandtranslate\n",
            "- connectedfeed\n",
            "- For\n",
            "- ŁukaszKaiserandSamyBengio\n",
            "- dluohs\n",
            "- Separable convolutions\n",
            "- suchattentionmechanisms\n",
            "- pairencoding\n",
            "- machinetranslationarchitectures\n",
            "- predictionsforpositionicandependonlyontheknownoutputsatpositionslessthani\n",
            "- JakobUszkoreit∗\n",
            "- arXivpreprintarXiv:1705.03122v2,2017\n",
            "- constant\n",
            "- Therearemanychoicesofpositionalencodings\n",
            "- relyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput\n",
            "- Thebigmodelsweretrainedfor300,000steps\n",
            "- ∈Rhdv×dmodel\n",
            "- weachieveanewstateoftheart\n",
            "- extremelysmallgradients4\n",
            "- NoamShazeer∗\n",
            "- pages832–841.ACL\n",
            "- an\n",
            "- Inadditiontoattentionsub\n",
            "- produceoutputsofdimensiond\n",
            "- sinceforanyfixedoffsetk\n",
            "- whereas\n",
            "- To\n",
            "- Thethirdisthepathlengthbetweenlong\n",
            "- not\n",
            "- Weusedavocabularyof16KtokensfortheWSJonlysettingandavocabularyof32Ktokens\n",
            "- tree\n",
            "- gnitov\n",
            "- havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n",
            "- Petrov\n",
            "- toO(k·n·d+n·d2\n",
            "- long\n",
            "- RafalJozefowicz\n",
            "- WearegratefultoNalKalchbrennerandStephanGouwsfortheirfruitful\n",
            "- block\n",
            "- InICLR,2015\n",
            "- ym\n",
            "- base\n",
            "- values\n",
            "- at\n",
            "- InProc.ofNAACL,2016\n",
            "- Inaddition\n",
            "- August2009\n",
            "- suchasimages\n",
            "- Thewavelengthsformageometricprogressionfrom2πto10000·2π\n",
            "- supervisedsetting\n",
            "- regressiveproperty\n",
            "- InTable3rows(A),wevarythenumberofattentionheadsandtheattentionkeyandvaluedimensions\n",
            "- Parser[29]evenwhentrainingonlyontheWSJtrainingsetof40Ksentences\n",
            "- theverb‘making’,completingthephrase‘making\n",
            "- whereposisthepositionandiisthedimension\n",
            "- Duetothereduceddimensionofeachhead\n",
            "- increasingthelengthofthelongestpaths\n",
            "- Fast\n",
            "- WO\n",
            "- forwardnetwork\n",
            "- M.Sugiyama\n",
            "- cid:80)dk\n",
            "- Bottom\n",
            "- ZhongqiangHuangandMaryHarper\n",
            "- acrosslanguages\n",
            "- bestmodelsfromtheliterature\n",
            "- corpusofenglish\n",
            "- werewrittenat10\n",
            "- allotherparameters\n",
            "- recognition\n",
            "- Operations\n",
            "- Table4\n",
            "- dataset\n",
            "- Francois\n",
            "- attentionandstarted\n",
            "- Transformer(basemodel\n",
            "- the\n",
            "- once\n",
            "- arXivpreprintarXiv:1705.04304,2017\n",
            "- trainingPCFGgrammarswithlatentannotations\n",
            "- plantoextendtheTransformertoproblemsinvolvinginputandoutputmodalitiesotherthantextand\n",
            "- representations\n",
            "- consists\n",
            "- Vinyals&Kaiser\n",
            "- andR.Garnett\n",
            "- indifferentways\n",
            "- Frenchtranslationtasks\n",
            "- ofgatedrecurrentneuralnetworksonsequencemodeling\n",
            "- tI\n",
            "- consisting\n",
            "- Proceedings of\n",
            "- simple\n",
            "- linearlyforConvS2SandlogarithmicallyforByteNet\n",
            "- equal\n",
            "- headsclearlylearntoperformdifferenttasks\n",
            "- statesh\n",
            "- textualentailmentandlearningtask\n",
            "- LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\n",
            "- sin(pos/100002i\n",
            "- eachdimensionofthepositionalencoding\n",
            "- andsemanticstructureofthesentences\n",
            "- Training\n",
            "- andLukaszKaiser\n",
            "- big\n",
            "- attentionlayerallofthekeys\n",
            "- layertransformerwithd\n",
            "- dessap\n",
            "- n\n",
            "- and\n",
            "- Weimplementthis\n",
            "- rateoverthecourseoftraining\n",
            "- sequential\n",
            "- Generating\n",
            "- Weusedbeamsearchasdescribedintheprevioussection\n",
            "- be\n",
            "- TrainingCost(FLOPs\n",
            "- Table2summarizesourresultsandcomparesourtranslationqualityandtrainingcoststoothermodel\n",
            "- computinghiddenrepresentationsinparallelforallinputandoutputpositions\n",
            "- nisthesequencelength\n",
            "- sequence(y\n",
            "- arXiv\n",
            "- fromtwodifferentheadsfromtheencoderself\n",
            "- audioandvideo\n",
            "- Deep residual\n",
            "- d\n",
            "- themaximumpathlengthbetweenanytwoinputandoutputpositionsinnetworkscomposedofthe\n",
            "- attentionlayers\n",
            "- dimensions\n",
            "- P\n",
            "- JournalofMachine\n",
            "- valuesandqueries\n",
            "- encoder\n",
            "- requires\n",
            "- moredifficult\n",
            "- bottomlineoftable3),steptimewas1.0seconds\n",
            "- layerinputandnormalized\n",
            "- Each layer\n",
            "- theoutputissubjecttostrongstructural\n",
            "- MachineTranslation\n",
            "- positioninthedecodertoattendoverallpositionsintheinputsequence\n",
            "- Learning\n",
            "- counteract\n",
            "- sequence\n",
            "- memory\n",
            "- whichisappliedtoeachpositionseparatelyandidentically\n",
            "- trained\n",
            "- Weneedtopreventleftward\n",
            "- sothatthetwocanbesummed\n",
            "- lukaszkaiser@google.com\n",
            "- productattentionbymaskingout(settingto−∞)allvaluesintheinput\n",
            "- describedinsection3.2\n",
            "- pointcapacityofeachGPU5\n",
            "- Separable\n",
            "- effortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder\n",
            "- determining\n",
            "- sequencelengths\n",
            "- usedsuccessfullyinavarietyoftasksincludingreadingcomprehension\n",
            "- InInternationalConference\n",
            "- eht\n",
            "- DoingsorequiresastackofO(n\n",
            "- fully\n",
            "- Dot\n",
            "- architectures\n",
            "- Weareexcitedaboutthefutureofattention\n",
            "- on\n",
            "- K\n",
            "- transductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5\n",
            "- it\n",
            "- per\n",
            "- MitchellPMarcus\n",
            "- editors\n",
            "- SergeyIoffe\n",
            "- longer\n",
            "- Our model\n",
            "- efficientinferenceandvisualizations\n",
            "- another\n",
            "- andtheinputforpositiont\n",
            "- Here\n",
            "- precisionfloating\n",
            "- In the\n",
            "- a\n",
            "- andJakobUszkoreit\n",
            "- siht\n",
            "- itself\n",
            "- References\n",
            "- attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9\n",
            "- EnglishConstituencyParsing\n",
            "- Neuralmachinetranslationofrarewords\n",
            "- learning\n",
            "- task\n",
            "- albeitatthecostofreducedeffectiveresolutiondue\n",
            "- ofthevalues\n",
            "- knowledge\n",
            "- size\n",
            "- results\n",
            "- VariationsontheTransformerarchitecture\n",
            "- entirelyonself\n",
            "- usingthelargerhigh\n",
            "- Huang&Harper(2009)[14\n",
            "- https://github.com/\n",
            "- Forourbasemodelsusing\n",
            "- recurrentnets\n",
            "- eachtrainingsteptookabout0.4seconds\n",
            "- sequences\n",
            "- increasedthemaximumoutputlengthtoinputlength+300\n",
            "- basedsolelyonattentionmechanisms\n",
            "- weuselearnedembeddingstoconverttheinput\n",
            "- traverseinthenetwork\n",
            "- computationalcomplexity\n",
            "- weusedasinglemodelobtainedbyaveragingthelast5checkpoints\n",
            "- final\n",
            "- Weemployaresidualconnection[11]aroundeachof\n",
            "- thetwomechanismsperformsimilarly\n",
            "- V)=Concat(head\n",
            "- sur-\n",
            "- machinetranslation\n",
            "- sizeofconvolutionsandrthesizeoftheneighborhoodinrestrictedself\n",
            "- abs/1412.3555,2014\n",
            "- InformationProcessingSystems,(NIPS),2016\n",
            "- pages3104–3112,2014\n",
            "- summarization\n",
            "- ssecorp\n",
            "- h\n",
            "- divideeachby\n",
            "- networkgrammars\n",
            "- Deep recurrent\n",
            "- Maximumpathlengths\n",
            "- basedmodelsandplantoapplythemtoothertasks\n",
            "- Evenourbasemodel\n",
            "- ndoesnotconnectallpairsofinputandoutput\n",
            "- InEmpiricalMethodsinNaturalLanguageProcessing,2016\n",
            "- andqueriescomefromthesameplace\n",
            "- tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\n",
            "- Attention as\n",
            "- DavidMcClosky\n",
            "- generates\n",
            "- short\n",
            "- machine\n",
            "- v\n",
            "- andAlexanderM.Rush\n",
            "- We trained\n",
            "- JakobproposedreplacingRNNswithself\n",
            "- positionalencodingsinboththeencoderanddecoderstacks\n",
            "- Xiv preprint\n",
            "- layer\n",
            "- layercomplexityandminimumnumberofsequentialoperations\n",
            "- dataregimes[37\n",
            "- ro\n",
            "- †WorkperformedwhileatGoogleBrain\n",
            "- constraintofsequentialcomputation\n",
            "- Inthiswork\n",
            "- andChristopherDManning\n",
            "- Recognition\n",
            "- PennTreebank[25],about40Ktrainingsentences\n",
            "- train\n",
            "- aidan@cs.toronto.edu\n",
            "- Numerous\n",
            "- dotproductattentionwithoutscalingforlargervaluesofd\n",
            "- relativepositions\n",
            "- single\n",
            "- E\n",
            "- including\n",
            "- remained\n",
            "- AttentionVisualizations\n",
            "- or\n",
            "- erom\n",
            "- amodelarchitectureeschewingrecurrenceandinstead\n",
            "- learned\n",
            "- case\n",
            "- convolutional\n",
            "- keys\n",
            "- inparticular\n",
            "- q·k=\n",
            "- Pattern\n",
            "- Thedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\n",
            "- allpositionsinthedecoderuptoandincludingthatposition\n",
            "- through\n",
            "- theinputoroutputsequences[2,19\n",
            "- InProceedingsofthe2009ConferenceonEmpiricalMethodsinNatural\n",
            "- are\n",
            "- wesharethesameweightmatrixbetweenthetwoembeddinglayersandthepre-√softmax\n",
            "- andthememorykeysandvaluescomefromtheoutputoftheencoder\n",
            "- LearningResearch,15(1):1929–1958,2014\n",
            "- TheTransformerachievesbetterBLEUscoresthanpreviousstate\n",
            "- hurtsperplexity\n",
            "- waL\n",
            "- Recentworkhasachieved\n",
            "- recurrent\n",
            "- IncontrasttoRNNsequence\n",
            "- forwardlayer\n",
            "- sequentialnatureprecludesparallelizationwithintrainingexamples\n",
            "- independentsentencerepresentations[4,27,28,22\n",
            "- Wealsousetheusuallearnedlineartransfor-\n",
            "- butno\n",
            "- Kaiming\n",
            "- Computationallinguistics,19(2):313–330,1993\n",
            "- attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\n",
            "- drop\n",
            "- Learningphraserepresentationsusingrnnencoder\n",
            "- ensuresthatthe\n",
            "- productattentionis\n",
            "- 4Toillustratewhythedotproductsgetlarge\n",
            "- wewilldescribetheTransformer\n",
            "- wherethequery\n",
            "- astheembeddings\n",
            "- N\n",
            "- rangedependencies[12\n",
            "- Convolutionallayersaregenerallymoreexpensivethan\n",
            "- Abstract\n",
            "- The\n",
            "- Onekeyfactoraffectingthe\n",
            "- Manyoftheattentionheadsexhibitbehaviourthatseemsrelatedtothestructureofthe\n",
            "- kernel\n",
            "- tionalsequencetosequencelearning\n",
            "- ourresearch\n",
            "- fromourmodelsandpresentanddiscussexamplesintheappendix\n",
            "- LC.sc\n",
            "- ToevaluatetheimportanceofdifferentcomponentsoftheTransformer\n",
            "- While\n",
            "- confidenceandBerkleyParsercorporafromwithapproximately17Msentences\n",
            "- Anattentionfunctioncanbedescribedasmappingaqueryandasetofkey\n",
            "- A\n",
            "- Inthesemodels\n",
            "- pairencoding[3],whichhasasharedsource-\n",
            "- embedding\n",
            "- artmodelsonthe\n",
            "- eachofthelayersinourencoderanddecodercontainsafully\n",
            "- operations\n",
            "- RecurrentNeuralNetworkGrammar[8\n",
            "- attentionlayersarefasterthanrecurrentlayerswhenthesequence\n",
            "- Eachpositionintheencodercanattendtoallpositionsinthepreviouslayerofthe\n",
            "- /h=64\n",
            "- describing\n",
            "- Encoder\n",
            "- biggermodelsarebetter\n",
            "- We\n",
            "- thematrixofoutputsas\n",
            "- motivate\n",
            "- networksformachine\n",
            "- noitartsiger\n",
            "- thecomplexityofaseparable\n",
            "- followed\n",
            "- Intheformertaskourbest\n",
            "- based\n",
            "- Thismakes\n",
            "- distance\n",
            "- Figure5\n",
            "- convolutions\n",
            "- models\n",
            "- wemustinjectsomeinformationabouttherelativeorabsolutepositionofthe\n",
            "- productattentionisidenticaltoouralgorithm\n",
            "- butimprovesaccuracyandBLEUscore\n",
            "- O(1\n",
            "- We propose\n",
            "- August2013\n",
            "- considerably\n",
            "- O(n2·d\n",
            "- Structuredattentionnetworks\n",
            "- In\"encoder\n",
            "- followedbylayernormalization\n",
            "- tsuj\n",
            "- shownintheleftandrighthalvesofFigure1\n",
            "- compact\n",
            "- orO(log\n",
            "- architecture\n",
            "- decoderarchitectureswith\n",
            "- In\n",
            "- Sequential\n",
            "- atafractionofthetrainingcostofanyof\n",
            "- q\n",
            "- and6\n",
            "- JonasGehring\n",
            "- wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\n",
            "- FFN(x)=max(0,xW\n",
            "- Effectiveself\n",
            "- Germantranslationonthe\n",
            "- To the\n",
            "- Our\n",
            "- O(r·n·d\n",
            "- PositionalEncoding\n",
            "- correspondstoasinusoid\n",
            "- Section\n",
            "- Furthermore\n",
            "- forwardconnectionsforneuralmachinetranslation\n",
            "- in\n",
            "- Wealsomodifytheself\n",
            "- attentionlayersrunninginparallel\n",
            "- beparallelized\n",
            "- Weusedabeamsizeof21andα=0.3\n",
            "- trainedthebasemodelsforatotalof100,000stepsor12hours\n",
            "- productattention\n",
            "- dependenciesisakeychallengeinmanysequencetransductiontasks\n",
            "- Listed\n",
            "- Frenchnewstest2014testsatafractionofthetrainingcost\n",
            "- RomainPaulus\n",
            "- asimplewaytopreventneuralnetworksfromoverfitting\n",
            "- InInternationalConferenceonLearningRepresentations,2017\n",
            "- FactorizationtricksforLSTMnetworks\n",
            "- During inference\n",
            "- That is\n",
            "- querywiththecorrespondingkey\n",
            "- abs/1512.00567,2015\n",
            "- andJeffDean\n",
            "- designedandimplementedthefirstTransformermodelsand\n",
            "- generative\n",
            "- NeuralGPUslearnalgorithms\n",
            "- connect\n",
            "- Inpractice\n",
            "- GNMT+RL[38\n",
            "- inference\n",
            "- also\n",
            "- Thiscorrespondstoincreasingthelearningratelinearlyforthefirstwarmup_stepstrainingsteps\n",
            "- allowingmodelingofdependencieswithoutregardtotheirdistancein\n",
            "- targetvocabularyofabout37000tokens\n",
            "- On both\n",
            "- Decoder\n",
            "- Attentionshereshownonlyfor\n",
            "- averaging\n",
            "- annotation\n",
            "- greatlyimprovingresultsandmassivelyaccelerating\n",
            "- difficult\n",
            "- dependencies\n",
            "- performs\n",
            "- InProceedingsofthe51stAnnualMeetingoftheACL(Volume\n",
            "- combinedwithfactthattheoutputembeddingsareoffsetbyoneposition\n",
            "- noam@google.com\n",
            "- andoutputsequences\n",
            "- Thetwomostcommonlyusedattentionfunctionsareadditiveattention[2],anddot\n",
            "- era\n",
            "- avaswani@google.com\n",
            "- encoderself\n",
            "- noinipo\n",
            "- bothattentionandresidual\n",
            "- relying\n",
            "- querywithallkeys\n",
            "- IlliaPolosukhin∗\n",
            "- V)=softmax\n",
            "- use\n",
            "- On\n",
            "- Isolatedattentionsfromjusttheword‘its’forattentionheads5\n",
            "- tokenprobabilities\n",
            "- freepositionrepresentationandbecametheotherpersoninvolvedinnearlyevery\n",
            "- DiederikKingmaandJimmyBa\n",
            "- 0.98andϵ=10−9\n",
            "- composed\n",
            "- Attention consists\n",
            "- wepresentedtheTransformer\n",
            "- attending\n",
            "- thetotalcomputationalcost\n",
            "- sequencetosequencelearning\n",
            "- compare\n",
            "- arXivpreprintarXiv:1701.06538,2017\n",
            "- andapplyasoftmaxfunctiontoobtaintheweightsonthe\n",
            "- 31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach\n",
            "- Amethodforstochasticoptimization\n",
            "- canberepresentedasalinearfunctionof\n",
            "- t\n",
            "- fitting\n",
            "- modelbymultiplyingthetrainingtime\n",
            "- Top\n",
            "- ProductAttention\"(Figure2\n",
            "- andoutputareallvectors\n",
            "- Nikidesigned\n",
            "- ∈Rdmodel×dv\n",
            "- product(multi-\n",
            "- Effectiveapproachestoattention-\n",
            "- You\n",
            "- pages433–440.ACL\n",
            "- tcefrep\n",
            "- weemployresidualconnections\n",
            "- Rethinkingtheinceptionarchitectureforcomputervision\n",
            "- Given\n",
            "- thenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\n",
            "- efficientinpractice\n",
            "- Wealsoexperimentedwithusinglearnedpositionalembeddings[9]instead\n",
            "- Whileforsmallvaluesofd thetwomechanismsperformsimilarly\n",
            "- of\n",
            "- kisthekernel\n",
            "- andthesecondisasimple\n",
            "- as\n",
            "- thedecoderinsertsathirdsub\n",
            "- pushingthesoftmaxfunctionintoregionswhereithas\n",
            "- Trainingtook3.5dayson8P100GPUs\n",
            "- however\n",
            "- wefounditbeneficialtolinearlyprojectthequeries\n",
            "- dev\n",
            "- arXiv:1703.10722,2017\n",
            "- Thisallowsevery\n",
            "- arXiv:1703.03130,2017\n",
            "- OfirPressandLiorWolf\n",
            "- translationsystem\n",
            "- Our results\n",
            "- Rd\n",
            "- for\n",
            "- thebigtransformermodel(Transformer(big\n",
            "- attentionis0.9BLEUworsethanthebestsetting\n",
            "- swal\n",
            "- tensor2tensor\n",
            "- Convolu-\n",
            "- ∗Equalcontribution\n",
            "- arXiv:1308.0850,2013\n",
            "- andYannN.Dauphin\n",
            "- Grammarasaforeignlanguage\n",
            "- AllmetricsareontheEnglish\n",
            "- Generating sequences\n",
            "- anddecreasingitthereafterproportionallytotheinversesquarerootofthestepnumber\n",
            "- attentionandpoint\n",
            "- arXivpreprintarXiv:1508.04025,2015\n",
            "- is\n",
            "- ModelArchitecture\n",
            "- sophisticated\n",
            "- subspacesatdifferentpositions\n",
            "- cos(pos/100002i\n",
            "- experts\n",
            "- Wecompute\n",
            "- these\n",
            "- attentionandtheparameter\n",
            "- assumethatthecomponentsofqandkareindependentrandom\n",
            "- headattentionallowsthemodeltojointlyattendtoinformationfromdifferentrepresentation\n",
            "- wescalethedotproductsby\n",
            "- various\n",
            "- standard\n",
            "- Thismimicsthe\n",
            "- Transformer(4layers\n",
            "- prevent\n",
            "- achieves\n",
            "- wecomputetheattentionfunctiononasetofqueriessimultaneously\n",
            "- matrixmultiplicationcode\n",
            "- asmemoryconstraintslimitbatchingacrossexamples\n",
            "- OleksiiKuchaievandBorisGinsburg\n",
            "- both\n",
            "- asthemodellearnstobemoreunsure\n",
            "- keysandvalueshtimeswithdifferent\n",
            "- Thistaskpresentsspecificchallenges\n",
            "- residual\n",
            "- computation[32],whilealsoimprovingmodelperformanceincaseofthelatter\n",
            "- dos\n",
            "- less\n",
            "- Experiments\n",
            "- fast\n",
            "- ehT\n",
            "- TrainingDataandBatching\n",
            "- hasbeencruciallyinvolvedineveryaspectofthiswork\n",
            "- specific\n",
            "- ConvS2S[9\n",
            "- theefforttoevaluatethisidea\n",
            "- headedself\n",
            "- ConvS2SEnsemble[9\n",
            "- EncoderandDecoderStacks\n",
            "- German translation\n",
            "- ofsymbolsoneelementatatime\n",
            "- This\n",
            "- positions\n",
            "- informationflowinthedecodertopreservetheauto\n",
            "- k)convolutionallayersinthecaseofcontiguouskernels\n",
            "- ŁukaszKaiserandIlyaSutskever\n",
            "- Fast and\n",
            "- Theshorterthesepathsbetweenanycombinationofpositionsintheinput\n",
            "- Noamproposedscaleddot\n",
            "- Inallbutafewcases[27],however\n",
            "- ThedecoderisalsocomposedofastackofN\n",
            "- Inadditiontothetwo\n",
            "- becauseitmayallowthemodeltoextrapolatetosequencelengthslongerthantheonesencountered\n",
            "- plicative)attention\n",
            "- Germantranslationdevelopmentset\n",
            "- hurtsmodelquality\n",
            "- work\n",
            "- compatibility\n",
            "- Weperformedonlyasmallnumberofexperimentstoselectthedropout\n",
            "- Aligningthepositionstostepsincomputationtime\n",
            "- Wesuspectthatforlargevaluesof\n",
            "- O(n·d2\n",
            "- decoderstructure[5,2,35\n",
            "- variableswithmean0andvariance1.Thentheirdotproduct\n",
            "- ApplicationsofAttentioninourModel\n",
            "- Table3\n",
            "- lineartransformation\n",
            "- age\n",
            "- ecnis\n",
            "- Another way\n",
            "- significantly\n",
            "- Thefundamental\n",
            "- position-\n",
            "- gatedmixture\n",
            "- attention\n",
            "- lengthsequenceofsymbolrepresentations\n",
            "- ytirojam\n",
            "- Providedproperattributionisprovided\n",
            "- Llionalsoexperimentedwithnovelmodelvariants\n",
            "- previousstate\n",
            "- wasresponsibleforourinitialcodebase\n",
            "- In this\n",
            "- section5.4),learningratesandbeamsizeontheSection22developmentset\n",
            "- restrictedattentionmechanismstoefficientlyhandlelargeinputsandoutputs\n",
            "- termdependencies,2001\n",
            "- pos,2i+1\n",
            "- thefirstsequencetransductionmodelbasedentirelyon\n",
            "- artBLEUscoreof41.8after\n",
            "- wisefeed\n",
            "- neural\n",
            "- wisefullyconnectedfeed\n",
            "- translationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs\n",
            "- VWV\n",
            "- betweenanytwopositionsinthenetwork\n",
            "- gnikam\n",
            "- theapproachwetakeinourmodel\n",
            "- code\n",
            "- steps\n",
            "- andRuslanSalakhutdi-\n",
            "- Att+PosUnk[39\n",
            "- functionthandotproductmaybebeneficial\n",
            "- AdvancesinNeuralInformationProcessingSystems,2015\n",
            "- abs/1406.1078,2014\n",
            "- Wecallourparticularattention\"ScaledDot\n",
            "- Notonlydoindividualattention\n",
            "- reading\n",
            "- attentioncouldyieldmoreinterpretablemodels\n",
            "- Withasingleattentionhead\n",
            "- fo\n",
            "- constituencyparsing\n",
            "- vocabulary[38].Sentencepairswerebatchedtogetherbyapproximatesequencelength\n",
            "- Results\n",
            "- decoder\n",
            "- manyappeartoexhibitbehaviorrelatedtothesyntactic\n",
            "- Weplantoinvestigatethisapproachfurtherinfuturework\n",
            "- Evenwithk\n",
            "- issimilartothatofsingle\n",
            "- usedbeamsearchwithabeamsizeof4andlengthpenaltyα\n",
            "- sinceitcanbeimplementedusinghighlyoptimized\n",
            "- Xception\n",
            "- Thisinherently\n",
            "- headattentioninthreedifferentways\n",
            "- weusesineandcosinefunctionsofdifferentfrequencies\n",
            "- accordingtoourbyte\n",
            "- preprint\n",
            "- OntheWMT2014English\n",
            "- All\n",
            "- alignedrecurrenceandhavebeenshowntoperformwellonsimple\n",
            "- then\n",
            "- Conference on\n",
            "- Self\n",
            "- Dropout\n",
            "- Neural computation\n",
            "- dot\n",
            "- weemployedlabelsmoothingofvalueϵ\n",
            "- WeshowthattheTransformergeneralizeswellto\n",
            "- dmodel\n",
            "- wemultiplythoseweightsby\n",
            "- ourbigmodelachievesaBLEUscoreof41.0\n",
            "- available\n",
            "- architectures[38,24,15\n",
            "- Layer5\n",
            "- Whilethelineartransformationsarethesameacrossdifferentpositions\n",
            "- Hochreiter and\n",
            "- TheTransformer(big)modeltrainedforEnglish\n",
            "- developmentset\n",
            "- masking\n",
            "- typical\n",
            "- Scaled\n",
            "- ThekeysandvaluesarealsopackedtogetherintomatricesK\n",
            "- replacingourearliercodebase\n",
            "- considerthreedesiderata\n",
            "- ModelVariations\n",
            "- Makinggenerationlesssequentialisanotherresearchgoalsofours\n",
            "- Bridgingthegapbetweenhumanandmachinetranslation\n",
            "- shift\n",
            "- pos,2i\n",
            "- ∈Rdmodel×dk\n",
            "- convolutionalneuralnetworksthatincludeanencoderandadecoder\n",
            "- C\n",
            "- Recurrent neural\n",
            "- 5Weusedvaluesof2.8,3.7,6.0and9.5TFLOPSforK80,K40,M40andP100,respectively\n",
            "- entirely\n",
            "- Background\n",
            "- Weemploythreetypesofregularizationduringtraining\n",
            "- surpassesallpreviouslypublishedmodelsandensembles\n",
            "- anddropoutisveryhelpfulinavoidingover\n",
            "- Chollet\n",
            "- Figure4\n",
            "- toinvestigatelocal\n",
            "- employ\n",
            "- German and\n",
            "- Weused\n",
            "- Long\n",
            "- identical\n",
            "- WhySelf\n",
            "- from\n",
            "- piece\n",
            "- ForwardNetworks\n",
            "- withsubwordunits\n",
            "- Outrageouslylargeneuralnetworks\n",
            "- An\n",
            "- andMarkJohnson\n",
            "- Forthebasemodel\n",
            "- wevariedourbasemodel\n",
            "- easy\n",
            "- layers\n",
            "- inthedistancebetweenpositions\n",
            "- m\n",
            "- correctionsandinspiration\n",
            "- depthwise\n",
            "- ProceedingsoftheHumanLanguageTechnologyConferenceoftheNAACL\n",
            "- An example\n",
            "- theygenerateasequenceofhidden\n",
            "- ew\n",
            "- warmup_steps=4000\n",
            "- German base\n",
            "- factor\n",
            "- byover2BLEU.OntheWMT2014English\n",
            "- Tothisend\n",
            "- SOE\n",
            "- following\n",
            "- Deep learning\n",
            "- reproducethetablesandfiguresinthispapersolelyforuseinjournalisticor\n",
            "- im-\n",
            "- Learning accurate\n",
            "- Wegivetwosuchexamplesabove\n",
            "- valuepairstoanoutput\n",
            "- suggests\n",
            "- andYoshuaBengio\n",
            "- whichperformsmulti\n",
            "- attentionlayersinthedecoderalloweachpositioninthedecodertoattendto\n",
            "- Inc.\n",
            "- decoderattention\"layers\n",
            "- hasmean0andvarianced\n",
            "- theeasieritistolearnlong\n",
            "- Toimprovecomputationalperformancefortasksinvolving\n",
            "- Sequencetosequencelearningwithneural\n",
            "- English-\n",
            "- distant\n",
            "- such\n",
            "- attentionlayerconnectsallpositionswithaconstantnumberofsequentially\n",
            "- improving\n",
            "- prisinglywell\n",
            "- Google’sneuralmachine\n",
            "- Theconfigurationofthismodelis\n",
            "- languagequestionansweringand\n",
            "- arXivpreprint\n",
            "- TheTransformer\n",
            "- Twoattentionheads\n",
            "- section\n",
            "- z\n",
            "- Buildingalargeannotated\n",
            "- dimensionality\n",
            "- andYonghuiWu\n",
            "- Tocounteractthiseffect\n",
            "- GNMT+RLEnsemble[38\n",
            "- r\n",
            "- most\n",
            "- LanguageProcessing\n",
            "- perplexitiesareper\n",
            "- concatenated\n",
            "- Neuralmachinetranslationbyjointly\n",
            "- theword‘making\n",
            "- In terms\n",
            "- chosethisfunctionbecausewehypothesizeditwouldallowthemodeltoeasilylearntoattendby\n",
            "- length\n",
            "- ofWSJ\n",
            "- Position\n",
            "- InTable3rows(B),weobservethatreducingtheattentionkeysized\n",
            "- WepresenttheseresultsinTable3\n",
            "- sub\n",
            "- x\n",
            "- abstractivesummarization\n",
            "- linearprojectionstod\n",
            "- V\n",
            "- k\n",
            "- packedtogether\n",
            "- layerinatypicalsequencetransductionencoderordecoder\n",
            "- Weestimatethenumberoffloatingpointoperationsusedtotraina\n",
            "- abs/1703.03906,2017\n",
            "- termmemory\n",
            "- terms\n",
            "- y\n",
            "- GoogleBrain\n",
            "- Mostcompetitiveneuralsequencetransductionmodelshaveanencoder\n",
            "- inorderforthemodeltomakeuseofthe\n",
            "- muchfasterandmorespace\n",
            "- Regularization\n",
            "- that\n",
            "- headattentionwithfulldimensionality\n",
            "- convolu-\n",
            "- wiseFeed\n",
            "- For each\n",
            "\n",
            "### MEASUREMENT\n",
            "- 1 m\n",
            "- 3202\n",
            "g\n",
            "- 4.5 m\n",
            "- 9002\n",
            "g\n",
            "- 512.\n",
            "m\n",
            "\n",
            "### LENGTH\n",
            "No entities of this type found.\n",
            "\n",
            "### ORG\n",
            "- atlayer5of6\n",
            "- Deep-Att+PosUnkEnsemble[39\n",
            "- PPL BLEU\n",
            "- DzmitryBahdanau\n",
            "- SeppHochreiter\n",
            "- Sepp Hochreiter\n",
            "- MikeSchuster\n",
            "- Notethattheattentionsareverysharpforthisword\n",
            "- AnnaGoldie\n",
            "- MaximKrikun\n",
            "- Convolutional O(k·n·d2\n",
            "- WMT\n",
            "- KyunghyunCho\n",
            "- Zhuetal\n",
            "- NoamShazeer\n",
            "- YuanCao\n",
            "- YoshuaBengio\n",
            "- AlexGraves\n",
            "- Multi-Head Attention\n",
            "- DipanjanDas\n",
            "- Dyeretal\n",
            "- YoonKim\n",
            "- Input-Input\n",
            "- JunyoungChung\n",
            "- GoogleBrain GoogleBrain GoogleResearch GoogleResearch\n",
            "- CaglarGulcehre\n",
            "- BLEU\n",
            "- Minh-ThangLuong\n",
            "- OriolVinyals\n",
            "- HolgerSchwenk\n",
            "- McCloskyetal\n",
            "- EmbeddingsandSoftmax\n",
            "- Ashish,withIllia\n",
            "- Forthebasemodels\n",
            "- GoogleResearch UniversityofToronto\n",
            "- CoRR\n",
            "- TheTransformerallowsforsignificantlymoreparallelizationandcanreachanewstateoftheartin\n",
            "- KarenSimonyan\n",
            "- the IEEE Conference on Computer Vision\n",
            "- CaimingXiong\n",
            "- HieuPham\n",
            "- AnkurParikh\n",
            "- andMirellaLapata\n",
            "- CarlDenton\n",
            "- AlexKrizhevsky\n",
            "- RicoSennrich\n",
            "- NalKalchbrenner\n",
            "- CA\n",
            "- LabelSmoothing Duringtraining\n",
            "- AaronvandenOord\n",
            "- AzaliaMirhoseini\n",
            "- Dot-Product Attention\n",
            "- BarryHaddow\n",
            "- KrzysztofMaziarz\n",
            "- LasseEspeholt\n",
            "- the 21st International Conference on\n",
            "ComputationalLinguisticsand44thAnnualMeetingoftheACL\n",
            "- ChristianSzegedy\n",
            "- Manyoftheattentionheadsattendtoadistantdependencyof\n",
            "- TheTransformerusesmulti\n",
            "- Luongetal\n",
            "- OscarTäckström\n",
            "- N d\n",
            "- AndyDavis\n",
            "- D.D.Lee\n",
            "- QinGao\n",
            "- BartvanMerrienboer\n",
            "- Thissectiondescribesthetrainingregimeforourmodels\n",
            "- KlausMacherey\n",
            "- PE\n",
            "- RNN\n",
            "- ComplexityperLayer\n",
            "- NitishSrivastava\n",
            "- InthisworkweproposetheTransformer\n",
            "- ÇaglarGülçehre\n",
            "- LuongHoang\n",
            "- 7 Conclusion\n",
            "Inthiswork\n",
            "- FethiBougares\n",
            "- DavidGrangier\n",
            "- MainConference\n",
            "- Neuralmachinetranslationinlineartime.arXivpreprintarXiv:1610.10099v2\n",
            "- Transformer\n",
            "- VincentVanhoucke\n",
            "- DenisYarats\n",
            "- MichaelAuli\n",
            "- KWK\n",
            "- IlyaSutskever\n",
            "- PaoloFrasconi\n",
            "\n",
            "\n",
            "Results saved to: extracted_entities.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import pdfplumber\n",
        "import spacy\n",
        "import re\n",
        "import pandas as pd\n",
        "import tempfile\n",
        "import os\n",
        "from io import BytesIO\n",
        "\n",
        "# Load the SpaCy model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"SpaCy model loaded successfully\")\n",
        "except OSError:\n",
        "    print(\"SpaCy model not found. Please download it using: python -m spacy download en_core_web_sm\")\n",
        "    nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Add scientific entity patterns\n",
        "try:\n",
        "    ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "    patterns = [\n",
        "        {\"label\": \"SPECIES\", \"pattern\": [{\"LOWER\": {\"REGEX\": \"[a-z]+\"}}], \"id\": \"scientific_species\"},\n",
        "        {\"label\": \"MEASUREMENT\", \"pattern\": [{\"SHAPE\": \"d+.d+\"}, {\"LOWER\": {\"IN\": [\"mm\", \"cm\", \"m\", \"kg\", \"g\"]}}]},\n",
        "        {\"label\": \"LENGTH\", \"pattern\": [{\"LOWER\": \"fork\"}, {\"LOWER\": \"length\"}]},\n",
        "    ]\n",
        "    ruler.add_patterns(patterns)\n",
        "    print(\"Entity patterns added successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error setting up entity ruler: {e}\")\n",
        "    if \"entity_ruler\" not in nlp.pipe_names:\n",
        "        ruler = nlp.add_pipe(\"entity_ruler\", name=\"entity_ruler\")\n",
        "\n",
        "# Function to extract text from PDF with better error handling\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    if pdf_file is None:\n",
        "        return \"No file provided.\"\n",
        "\n",
        "    try:\n",
        "        # Save uploaded file to a temporary file\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp:\n",
        "            if isinstance(pdf_file, bytes):\n",
        "                tmp.write(pdf_file)\n",
        "            else:\n",
        "                tmp.write(pdf_file.read())\n",
        "            tmp_path = tmp.name\n",
        "\n",
        "        print(f\"Temporary PDF saved to: {tmp_path}\")\n",
        "\n",
        "        text = \"\"\n",
        "        try:\n",
        "            with pdfplumber.open(tmp_path) as pdf:\n",
        "                for page in pdf.pages:\n",
        "                    extracted_text = page.extract_text()\n",
        "                    if extracted_text:\n",
        "                        text += extracted_text + \"\\n\"\n",
        "                    print(f\"Extracted page with {len(extracted_text) if extracted_text else 0} characters\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error in pdfplumber: {str(e)}\")\n",
        "            return f\"Error extracting text from PDF: {str(e)}\"\n",
        "        finally:\n",
        "            # Clean up the temporary file\n",
        "            try:\n",
        "                os.unlink(tmp_path)\n",
        "                print(\"Temporary file cleaned up\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to clean up temp file: {e}\")\n",
        "\n",
        "        if not text.strip():\n",
        "            return \"No text could be extracted from the PDF.\"\n",
        "\n",
        "        print(f\"Successfully extracted {len(text)} characters from PDF\")\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"General error in extract_text_from_pdf: {str(e)}\")\n",
        "        return f\"Error processing PDF: {str(e)}\"\n",
        "\n",
        "# Function to extract entities with better error handling\n",
        "def extract_entities(text, entity_types):\n",
        "    if isinstance(text, str) and not text.startswith(\"Error\"):\n",
        "        try:\n",
        "            # Limit text size to avoid memory issues\n",
        "            text = text[:100000]  # Limit to first 100k characters\n",
        "            print(f\"Processing {len(text)} characters for entity extraction\")\n",
        "\n",
        "            doc = nlp(text)\n",
        "            print(f\"SpaCy document created with {len(doc)} tokens\")\n",
        "\n",
        "            # Create a dictionary to store extracted entities\n",
        "            extracted = {entity_type: [] for entity_type in entity_types}\n",
        "\n",
        "            # Extract required entity types\n",
        "            for ent in doc.ents:\n",
        "                if ent.label_ in entity_types:\n",
        "                    extracted[ent.label_].append(ent.text)\n",
        "\n",
        "            # Special case for scientific species names\n",
        "            if \"SPECIES\" in entity_types:\n",
        "                scientific_names = re.findall(r'[A-Z][a-z]+ [a-z]+', text)\n",
        "                extracted[\"SPECIES\"].extend(scientific_names)\n",
        "                print(f\"Found {len(scientific_names)} potential scientific species names\")\n",
        "\n",
        "            # Special case for measurements\n",
        "            if \"MEASUREMENT\" in entity_types:\n",
        "                measurements = re.findall(r'\\d+\\.?\\d*\\s*(?:mm|cm|m|kg|g)', text)\n",
        "                extracted[\"MEASUREMENT\"].extend(measurements)\n",
        "                print(f\"Found {len(measurements)} potential measurements\")\n",
        "\n",
        "            # Special case for fork length\n",
        "            if \"LENGTH\" in entity_types:\n",
        "                fork_lengths = re.findall(r'(?:fork|total)?\\s*length\\s*(?:of)?\\s*\\d+\\.?\\d*\\s*(?:mm|cm|m)', text, re.IGNORECASE)\n",
        "                extracted[\"LENGTH\"].extend(fork_lengths)\n",
        "                print(f\"Found {len(fork_lengths)} potential length measurements\")\n",
        "\n",
        "            # Remove duplicates\n",
        "            for entity_type in entity_types:\n",
        "                extracted[entity_type] = list(set(extracted[entity_type]))\n",
        "                print(f\"Found {len(extracted[entity_type])} unique {entity_type} entities\")\n",
        "\n",
        "            return extracted\n",
        "        except Exception as e:\n",
        "            print(f\"Error in extract_entities: {str(e)}\")\n",
        "            return f\"Error extracting entities: {str(e)}\"\n",
        "    else:\n",
        "        print(f\"Cannot extract entities from invalid text\")\n",
        "        return f\"Cannot extract entities: {text}\"\n",
        "\n",
        "# Function for the Gradio interface with extensive error handling and debugging\n",
        "# Function for the Gradio interface with extensive error handling and debugging\n",
        "def process_pdf(pdf_file, species, measurements, length, person, org, date, gpe, loc):\n",
        "    # ... (previous code: PDF check, entity selection, text extraction, entity extraction) ...\n",
        "\n",
        "    try:\n",
        "        # Extract text from PDF\n",
        "        text = extract_text_from_pdf(pdf_file)\n",
        "\n",
        "        # ... (text error check) ...\n",
        "\n",
        "        # Extract entities\n",
        "        extracted = extract_entities(text, entity_types)\n",
        "\n",
        "        # ... (extracted error check) ...\n",
        "\n",
        "        # Prepare results\n",
        "        result_text = \"## Extracted Entities\\n\\n\"\n",
        "        excel_file_path = None # Initialize file path variable\n",
        "\n",
        "        try:\n",
        "            # Create Excel file in memory first\n",
        "            output = BytesIO()\n",
        "            with pd.ExcelWriter(output, engine='xlsxwriter') as writer:\n",
        "                has_data = False # Flag to check if any data was written\n",
        "                for entity_type in entity_types:\n",
        "                    if extracted[entity_type]:\n",
        "                        has_data = True\n",
        "                        result_text += f\"### {entity_type}\\n\"\n",
        "                        for item in extracted[entity_type]:\n",
        "                            result_text += f\"- {item}\\n\"\n",
        "                        result_text += \"\\n\"\n",
        "\n",
        "                        # Add sheet to Excel file\n",
        "                        df = pd.DataFrame({entity_type: extracted[entity_type]})\n",
        "                        # Ensure sheet name is valid (Excel limits to 31 chars, avoid certain chars)\n",
        "                        safe_sheet_name = re.sub(r'[\\\\/*?:\\[\\]]', '_', entity_type)[:31]\n",
        "                        df.to_excel(writer, sheet_name=safe_sheet_name, index=False)\n",
        "                        print(f\"Added {len(extracted[entity_type])} {entity_type} entities to Excel buffer\")\n",
        "                    else:\n",
        "                        result_text += f\"### {entity_type}\\n\"\n",
        "                        result_text += \"No entities of this type found.\\n\\n\"\n",
        "\n",
        "            if has_data:\n",
        "                output.seek(0)\n",
        "                print(\"Excel file created in memory buffer\")\n",
        "\n",
        "                # --- CHANGE START ---\n",
        "                # Save the buffer to a temporary file instead of returning bytes\n",
        "                try:\n",
        "                    # Create a temporary file that Gradio can access\n",
        "                    # delete=False is important so the file isn't removed before Gradio reads it\n",
        "                    with tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx', mode='wb') as tmp_excel:\n",
        "                        tmp_excel.write(output.getvalue())\n",
        "                        excel_file_path = tmp_excel.name # Get the path to the temp file\n",
        "                    print(f\"Excel file saved temporarily to: {excel_file_path}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error saving temporary Excel file: {str(e)}\")\n",
        "                    result_text += f\"\\n\\n**Error:** Could not save Excel file for download: {str(e)}\"\n",
        "                    excel_file_path = None # Ensure path is None on error\n",
        "                # --- CHANGE END ---\n",
        "            else:\n",
        "                 print(\"No data found for any selected entity type. Excel file not generated.\")\n",
        "                 result_text += \"\\n\\n**Note:** No data found for selected types, so no Excel file was generated.\"\n",
        "\n",
        "\n",
        "            # Return the result text and the PATH to the file (or None)\n",
        "            return result_text, excel_file_path\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating Excel file content: {str(e)}\")\n",
        "            return f\"Error creating Excel file content: {str(e)}\", None # Return None for the file path on error\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error in process_pdf: {str(e)}\")\n",
        "        return f\"Unexpected error: {str(e)}\", None # Return None for the file path on error\n",
        "# Create Gradio interface\n",
        "def create_gradio_interface():\n",
        "    with gr.Blocks(title=\"Scientific Entity Extraction Tool\") as demo:\n",
        "        gr.Markdown(\"# Scientific Entity Extraction Tool\")\n",
        "        gr.Markdown(\"Upload a scientific PDF and select entities to extract\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                # Input components\n",
        "                pdf_input = gr.File(label=\"Upload PDF Document\", type=\"binary\")\n",
        "\n",
        "                gr.Markdown(\"### Select Entities to Extract\")\n",
        "                species_cb = gr.Checkbox(label=\"Species Names\", value=True)\n",
        "                measurements_cb = gr.Checkbox(label=\"Measurements\", value=True)\n",
        "                length_cb = gr.Checkbox(label=\"Length Measurements\", value=True)\n",
        "                person_cb = gr.Checkbox(label=\"Person Names\", value=False)\n",
        "                org_cb = gr.Checkbox(label=\"Organizations\", value=False)\n",
        "                date_cb = gr.Checkbox(label=\"Dates\", value=False)\n",
        "                gpe_cb = gr.Checkbox(label=\"Geopolitical Entities\", value=False)\n",
        "                loc_cb = gr.Checkbox(label=\"Locations\", value=False)\n",
        "\n",
        "                extract_button = gr.Button(\"Extract Entities\")\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                # Output components\n",
        "                output_text = gr.Markdown()\n",
        "                output_file = gr.File(label=\"Download Results\")\n",
        "\n",
        "        # Set up event handler\n",
        "        extract_button.click(\n",
        "            process_pdf,\n",
        "            inputs=[\n",
        "                pdf_input,\n",
        "                species_cb, measurements_cb, length_cb,\n",
        "                person_cb, org_cb, date_cb, gpe_cb, loc_cb\n",
        "            ],\n",
        "            outputs=[output_text, output_file]\n",
        "        )\n",
        "\n",
        "        # Add debugging information\n",
        "        gr.Markdown(\"### Debug Information\")\n",
        "        debug_output = gr.Textbox(label=\"Debug Log\", lines=5)\n",
        "\n",
        "        # Override print to capture debug output\n",
        "        original_print = print\n",
        "        def debug_print(*args, **kwargs):\n",
        "            original_print(*args, **kwargs)\n",
        "            message = \" \".join(str(arg) for arg in args)\n",
        "            debug_output.update(message + \"\\n\" + debug_output.value)\n",
        "\n",
        "        # Not actually overriding print, but demonstrating the concept\n",
        "        extract_button.click(\n",
        "            lambda: \"Starting extraction...\",\n",
        "            inputs=None,\n",
        "            outputs=debug_output\n",
        "        )\n",
        "\n",
        "    return demo\n",
        "\n",
        "# Launch the app\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting Scientific Entity Extraction Tool\")\n",
        "    demo = create_gradio_interface()\n",
        "    demo.launch(debug=True)  # Enable debug mode\n",
        "    print(\"Gradio interface launched\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tKqPrzbHHFBa",
        "outputId": "acda38fa-a814-4dba-d741-f6fb8b509f77"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpaCy model loaded successfully\n",
            "Entity patterns added successfully\n",
            "Starting Scientific Entity Extraction Tool\n",
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://45019d4570965a942b.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://45019d4570965a942b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temporary PDF saved to: /tmp/tmpsvvtzv6m.pdf\n",
            "Extracted page with 2580 characters\n",
            "Extracted page with 3778 characters\n",
            "Extracted page with 1616 characters\n",
            "Extracted page with 2216 characters\n",
            "Extracted page with 2843 characters\n",
            "Extracted page with 3066 characters\n",
            "Extracted page with 2921 characters\n",
            "Extracted page with 2766 characters\n",
            "Extracted page with 2694 characters\n",
            "Extracted page with 2803 characters\n",
            "Extracted page with 2963 characters\n",
            "Extracted page with 2934 characters\n",
            "Extracted page with 775 characters\n",
            "Extracted page with 777 characters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted page with 779 characters\n",
            "Temporary file cleaned up\n",
            "Successfully extracted 35526 characters from PDF\n",
            "Processing 35526 characters for entity extraction\n",
            "SpaCy document created with 4578 tokens\n",
            "Found 53 potential scientific species names\n",
            "Found 5 potential measurements\n",
            "Found 0 potential length measurements\n",
            "Found 1188 unique SPECIES entities\n",
            "Found 5 unique MEASUREMENT entities\n",
            "Found 0 unique LENGTH entities\n",
            "Found 87 unique ORG entities\n",
            "Added 1188 SPECIES entities to Excel buffer\n",
            "Added 5 MEASUREMENT entities to Excel buffer\n",
            "Added 87 ORG entities to Excel buffer\n",
            "Excel file created in memory buffer\n",
            "Excel file saved temporarily to: /tmp/tmpz48az_7l.xlsx\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7863 <> https://45019d4570965a942b.gradio.live\n",
            "Gradio interface launched\n"
          ]
        }
      ]
    }
  ]
}